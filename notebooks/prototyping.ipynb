{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate GPU is available for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 15 01:26:07 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def get_dataset(dataset):\n",
    "    ds = datasets.load_dataset('glue', dataset)\n",
    "    num_classes = ds['train'].features['label'].num_classes\n",
    "    return ds, num_classes\n",
    "\n",
    "\n",
    "def create_dataset_from_text_dataset(ds, tokenizer):\n",
    "    encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attn_masks = encoding['attention_mask']\n",
    "    labels = torch.tensor(ds['label'])\n",
    "    return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "\n",
    "def get_tensor_datasets(dataset_dict, splits, tokenizer):\n",
    "    split_datasets = {}\n",
    "    for s in splits:\n",
    "        split_datasets[s] = create_dataset_from_text_dataset(dataset_dict[s], tokenizer)\n",
    "    return split_datasets\n",
    "\n",
    "\n",
    "def get_data_loaders(split_datasets, batch_size):\n",
    "    train_loader = DataLoader(split_datasets['train'], batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(split_datasets['validation'], batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(split_datasets['test'], batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sst2 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = 'sst2'\n",
    "print(f'Loading {dataset} dataset...')\n",
    "dataset_dict, num_classes = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AdapterType\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_transformer(model_name, num_labels, adapter, dataset):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        if adapter:\n",
    "            model.add_adapter(dataset, AdapterType.text_task)\n",
    "            model.train_adapter(dataset)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_criterion(num_labels):\n",
    "    if num_labels == 2:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for bert-base-uncased...\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "print(f'Loading tokenizer for {model_name}...')\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader for various splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loader for ['train', 'validation', 'test'] splits...\n"
     ]
    }
   ],
   "source": [
    "# Create data loader for each split\n",
    "splits = list(dataset_dict.keys())\n",
    "print(f'Creating data loader for {splits} splits...')\n",
    "split_datasets = get_tensor_datasets(dataset_dict, splits, tokenizer)\n",
    "train_loader, val_loader, test_loader = get_data_loaders(split_datasets, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(train_loader):\n",
    "#     print(i_batch, sample_batched[2].size())\n",
    "    \n",
    "#     if i_batch == 0:\n",
    "#         print(sample_batched[2])\n",
    "#         sb = sample_batched[2].to('cuda')\n",
    "#         print(sb)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data loader\n",
    "# sample_loader = DataLoader(split_datasets['train'], batch_size=3, shuffle=True)\n",
    "# for i in sample_loader:\n",
    "#     input_ids, attn_masks, labels = i\n",
    "#     decoded = tokenizer.batch_decode(input_ids)\n",
    "#     for d in decoded:\n",
    "#         print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased with adapters=True...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "adapter = True\n",
    "print(f'Loading {model_name} with adapters={adapter}...')\n",
    "model = get_transformer(model_name,\n",
    "                        num_labels=num_classes,\n",
    "                        adapter=adapter,\n",
    "                        dataset=dataset)\n",
    "criterion = get_criterion(num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Learning Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_learning_scheme(learning_scheme, model, learning_rate, adapter, epoch):\n",
    "    if learning_scheme == 'differential':\n",
    "        optimizer_grouped_parameters = differential_learning_scheme(model, learning_rate)\n",
    "        optimizer = torch.optim.SGD(optimizer_grouped_parameters)\n",
    "    elif learning_scheme == 'fixed':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif learning_scheme == 'nesterov':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif learning_scheme == 'gradual-unfreeze':\n",
    "        optimizer_grouped_parameters = gradual_unfreezing_learning_scheme(model, learning_rate, adapter, epoch)\n",
    "        optimizer = torch.optim.SGD(optimizer_grouped_parameters)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def differential_learning_scheme(model, learning_rate=0.1, divisor=2.6):\n",
    "    param_prefixes = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "            if base not in param_prefixes:\n",
    "                param_prefixes[base] = 0\n",
    "\n",
    "    param_prefix_divisors = list(reversed([divisor * i for i in range(1, len(param_prefixes))])) + [1]\n",
    "    param_learning_rates = [learning_rate / ld for ld in param_prefix_divisors]\n",
    "\n",
    "    param_prefix_lr_lookup = dict(zip(param_prefixes.keys(), param_learning_rates))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': p, 'lr': param_prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "        for n, p in model.named_parameters() if p.requires_grad\n",
    "    ]\n",
    "\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def gradual_unfreezing_learning_scheme(model, learning_rate, adapter, epoch=1):\n",
    "    trainable_layers = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "            if adapter:\n",
    "                if base not in trainable_layers and 'adapter' or 'classifier' in base:\n",
    "                    trainable_layers.append(base)\n",
    "            else:\n",
    "                if base not in trainable_layers:\n",
    "                    trainable_layers.append(base)\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': p, 'lr': learning_rate}\n",
    "        for n, p in model.named_parameters() if p.requires_grad and n.partition('.weight')[0].partition('.bias')[0] in trainable_layers[-epoch:]\n",
    "    ]\n",
    "\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def get_scheduler(scheduler, optimizer, learning_rate, max_lr):\n",
    "    if scheduler:\n",
    "        if scheduler == 'cyclic-triangular':\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
    "                                                          base_lr=learning_rate,\n",
    "                                                          max_lr=max_lr,\n",
    "                                                          mode='triangular')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring differential learning scheme...\n"
     ]
    }
   ],
   "source": [
    "# Get learning scheme\n",
    "learning_scheme = 'differential'\n",
    "print(f'Configuring {learning_scheme} learning scheme...')\n",
    "optimizer = get_learning_scheme(learning_scheme, model, learning_rate=0.01, adapter=adapter, epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jupyter/coms6998-project/trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils.learning_scheme import get_learning_scheme\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, n_epochs, optimizer, scheduler, criterion, learning_scheme, learning_rate, adapter):\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.criterion = criterion.to(self.device)\n",
    "        self.learning_scheme = learning_scheme\n",
    "        self.learning_rate = learning_rate\n",
    "        self.adapter = adapter\n",
    "\n",
    "    def measure_performance(self, loader):\n",
    "        running_loss = 0.0\n",
    "        correct_count = 0.0\n",
    "        total_count = 0.0\n",
    "        for data in loader:\n",
    "            input_ids = data[0].to(self.device)\n",
    "            attn_masks = data[1].to(self.device)\n",
    "            labels = data[2].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                probas = F.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probas, axis=1)\n",
    "\n",
    "                # Track stats\n",
    "                running_loss += loss\n",
    "                correct_count += torch.sum(preds == labels)\n",
    "                total_count += len(labels)\n",
    "\n",
    "        running_loss /= len(loader)\n",
    "        acc = correct_count / total_count\n",
    "\n",
    "        return running_loss, acc\n",
    "\n",
    "    def train_loop(self, train_loader, val_loader, batch_logging=10):\n",
    "        print('Starting training loop')\n",
    "\n",
    "        print('Initial evaluating on validation dataset')\n",
    "        train_loss, train_acc = self.measure_performance(train_loader)\n",
    "        val_loss, val_acc = self.measure_performance(val_loader)\n",
    "        epoch_summary = f'[Epoch 0] | Train acc: {train_acc:.4f} Train loss: {train_loss:.4f} Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}'\n",
    "        print(epoch_summary)\n",
    "\n",
    "        epoch_history = [{'epoch': 0,\n",
    "                          'train loss': train_loss.item(),\n",
    "                          'train accuracy': train_acc.item(),\n",
    "                          'validation loss': val_loss.item(),\n",
    "                          'validation accuracy': val_acc.item(),\n",
    "                          'epoch time': 0}]\n",
    "        batch_history = [{'epoch': 0,\n",
    "                          'batch': 0,\n",
    "                          'train loss': train_loss.item(),\n",
    "                          'train accuracy': train_acc.item(),\n",
    "                          'validation loss': val_loss.item(),\n",
    "                          'validation accuracy': val_acc.item(),\n",
    "                          'batch time': 0}]\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "\n",
    "            if self.learning_scheme == 'gradual-unfreeze':\n",
    "                self.optimizer = get_learning_scheme(self.learning_scheme,\n",
    "                                                     self.model,\n",
    "                                                     self.learning_rate,\n",
    "                                                     self.adapter,\n",
    "                                                     epoch+1)\n",
    "\n",
    "            print(f'--- Epoch: {epoch+1} ---')\n",
    "            epoch_start_time = time.time()\n",
    "            batch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            total_count = 0.0\n",
    "\n",
    "            for i, data in enumerate(train_loader):\n",
    "                input_ids = data[0].to(self.device)\n",
    "                attn_masks = data[1].to(self.device)\n",
    "                labels = data[2].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Evaluation/optimization step\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                probas = F.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probas, axis=1)\n",
    "                running_acc += torch.sum(preds == labels).item()\n",
    "                total_count += len(labels)\n",
    "\n",
    "                # Print/log statistics periodically\n",
    "                if i % batch_logging == batch_logging - 1:\n",
    "                    batch_end_time = time.time()\n",
    "                    total_batch_time = batch_end_time - batch_start_time\n",
    "                    batch_loss = running_loss / batch_logging\n",
    "                    batch_acc = running_acc / total_count\n",
    "                    batch_val_loss, batch_val_acc = self.measure_performance(val_loader)\n",
    "\n",
    "                    batch_history.append({'epoch': epoch+1,\n",
    "                                          'batch': i + 1,\n",
    "                                          'train loss': batch_loss,\n",
    "                                          'train accuracy': batch_acc,\n",
    "                                          'validation loss': batch_val_loss.item(),\n",
    "                                          'validation accuracy': batch_val_acc.item(),\n",
    "                                          'batch time': total_batch_time})\n",
    "\n",
    "                    print(\n",
    "                        f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "                        f'Loss: {batch_loss:.5f} ',\n",
    "                        f'Acc: {batch_acc} ',\n",
    "                        f'Time: {total_batch_time:.2f} ',\n",
    "                        f'LR: {self.scheduler.get_last_lr()}' if self.scheduler else '')\n",
    "\n",
    "                    # Reset statistics\n",
    "                    batch_start_time = time.time()\n",
    "                    running_loss = 0.0\n",
    "                    running_acc = 0.0\n",
    "                    total_count = 0.0\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            total_epoch_time = epoch_end_time - epoch_start_time\n",
    "            train_loss, train_acc = self.measure_performance(train_loader)\n",
    "            val_loss, val_acc = self.measure_performance(val_loader)\n",
    "            epoch_summary = f'[Epoch {epoch + 1}] {total_epoch_time:.2f} seconds'\n",
    "            epoch_summary += f' | Train acc: {train_acc:.4f} Train loss: {train_loss:.4f} Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}'\n",
    "\n",
    "            epoch_history.append({'epoch': epoch + 1,\n",
    "                                  'train loss': train_loss.item(),\n",
    "                                  'train accuracy': train_acc.item(),\n",
    "                                  'validation loss': val_loss.item(),\n",
    "                                  'validation accuracy': val_acc.item(),\n",
    "                                  'epoch time': total_epoch_time})\n",
    "\n",
    "            print(epoch_summary)\n",
    "\n",
    "        print('Finished training')\n",
    "\n",
    "        return pd.DataFrame(epoch_history), pd.DataFrame(batch_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  n_epochs=5,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=None,\n",
    "                  criterion=criterion,\n",
    "                  learning_scheme=learning_scheme,\n",
    "                  learning_rate=0.01,\n",
    "                  adapter=adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 1 ---\n",
      "[CLS] hide new secretions from the parental units [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 2 ---\n",
      "[CLS] contains no wit, only labored gags [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 3 ---\n",
      "[CLS] that loves its characters and communicates something rather beautiful about human nature [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 4 ---\n",
      "[CLS] remains utterly satisfied to remain the same throughout [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 5 ---\n",
      "[CLS] on the worst revenge - of - the - nerds cliches the filmmakers could dredge up [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "sample_loader = DataLoader(split_datasets['train'], batch_size=5, shuffle=False)\n",
    "for i in sample_loader:\n",
    "    input_ids, attn_masks, labels = i\n",
    "    decoded = tokenizer.batch_decode(input_ids)\n",
    "    for j, d in enumerate(decoded):\n",
    "        print(f'--- Sample {j+1} ---')\n",
    "        print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n",
      "Initial evaluating on validation dataset\n",
      "[Epoch 0] | Train acc: 0.5577 Train loss: 0.6943 Val acc: 0.5092 Val loss: 0.7242\n",
      "--- Epoch: 1 ---\n",
      "[E1 B100]  Loss: 0.69384  Acc: 0.5421875  Time: 13.78  \n",
      "[E1 B200]  Loss: 0.68137  Acc: 0.5746875  Time: 13.78  \n",
      "[E1 B300]  Loss: 0.66610  Acc: 0.6025  Time: 13.79  \n",
      "[E1 B400]  Loss: 0.64029  Acc: 0.6303125  Time: 13.78  \n",
      "[E1 B500]  Loss: 0.63395  Acc: 0.648125  Time: 13.77  \n",
      "[E1 B600]  Loss: 0.62413  Acc: 0.6459375  Time: 13.78  \n",
      "[E1 B700]  Loss: 0.60315  Acc: 0.6796875  Time: 13.77  \n",
      "[E1 B800]  Loss: 0.60733  Acc: 0.6825  Time: 13.78  \n",
      "[E1 B900]  Loss: 0.59183  Acc: 0.7009375  Time: 13.78  \n",
      "[E1 B1000]  Loss: 0.57802  Acc: 0.71  Time: 13.77  \n",
      "[E1 B1100]  Loss: 0.58295  Acc: 0.700625  Time: 13.77  \n",
      "[E1 B1200]  Loss: 0.56639  Acc: 0.7265625  Time: 13.77  \n",
      "[E1 B1300]  Loss: 0.55560  Acc: 0.74125  Time: 13.77  \n",
      "[E1 B1400]  Loss: 0.55803  Acc: 0.7303125  Time: 13.77  \n",
      "[E1 B1500]  Loss: 0.55543  Acc: 0.7265625  Time: 13.76  \n",
      "[E1 B1600]  Loss: 0.53837  Acc: 0.7478125  Time: 13.77  \n",
      "[E1 B1700]  Loss: 0.53117  Acc: 0.7509375  Time: 13.78  \n",
      "[E1 B1800]  Loss: 0.52782  Acc: 0.755  Time: 13.78  \n",
      "[E1 B1900]  Loss: 0.52041  Acc: 0.7659375  Time: 13.77  \n",
      "[E1 B2000]  Loss: 0.52007  Acc: 0.759375  Time: 13.77  \n",
      "[E1 B2100]  Loss: 0.50850  Acc: 0.75875  Time: 13.77  \n",
      "[Epoch 1] 322.61 seconds | Train acc: 0.7614 Train loss: 0.5032 Val acc: 0.7775 Val loss: 0.5023\n",
      "--- Epoch: 2 ---\n",
      "[E2 B100]  Loss: 0.49990  Acc: 0.7746875  Time: 13.77  \n",
      "[E2 B200]  Loss: 0.50502  Acc: 0.764375  Time: 13.77  \n",
      "[E2 B300]  Loss: 0.49134  Acc: 0.788125  Time: 13.77  \n",
      "[E2 B400]  Loss: 0.49433  Acc: 0.7740625  Time: 13.77  \n",
      "[E2 B500]  Loss: 0.48278  Acc: 0.7828125  Time: 13.77  \n",
      "[E2 B600]  Loss: 0.48434  Acc: 0.7834375  Time: 13.77  \n",
      "[E2 B700]  Loss: 0.47334  Acc: 0.79875  Time: 13.77  \n",
      "[E2 B800]  Loss: 0.45913  Acc: 0.8021875  Time: 13.77  \n",
      "[E2 B900]  Loss: 0.46262  Acc: 0.7953125  Time: 13.77  \n",
      "[E2 B1000]  Loss: 0.46816  Acc: 0.7915625  Time: 13.77  \n",
      "[E2 B1100]  Loss: 0.46501  Acc: 0.7859375  Time: 13.77  \n",
      "[E2 B1200]  Loss: 0.44781  Acc: 0.7953125  Time: 13.77  \n",
      "[E2 B1300]  Loss: 0.45534  Acc: 0.7859375  Time: 13.78  \n",
      "[E2 B1400]  Loss: 0.45821  Acc: 0.7928125  Time: 13.77  \n",
      "[E2 B1500]  Loss: 0.45063  Acc: 0.7990625  Time: 13.76  \n",
      "[E2 B1600]  Loss: 0.43246  Acc: 0.81125  Time: 13.76  \n",
      "[E2 B1700]  Loss: 0.43374  Acc: 0.8103125  Time: 13.77  \n",
      "[E2 B1800]  Loss: 0.43066  Acc: 0.8109375  Time: 13.77  \n",
      "[E2 B1900]  Loss: 0.43217  Acc: 0.8078125  Time: 13.77  \n",
      "[E3 B1100]  Loss: 0.39347  Acc: 0.830625  Time: 13.77  \n",
      "[E3 B1200]  Loss: 0.41453  Acc: 0.8159375  Time: 13.76  \n",
      "[E3 B1300]  Loss: 0.38306  Acc: 0.828125  Time: 13.76  \n",
      "[E3 B1400]  Loss: 0.39674  Acc: 0.8325  Time: 13.77  \n",
      "[E3 B1500]  Loss: 0.38354  Acc: 0.826875  Time: 13.77  \n",
      "[E3 B1600]  Loss: 0.40447  Acc: 0.815625  Time: 13.76  \n",
      "[E3 B1700]  Loss: 0.40109  Acc: 0.83  Time: 13.76  \n",
      "[E3 B1800]  Loss: 0.38028  Acc: 0.834375  Time: 13.76  \n",
      "[E3 B1900]  Loss: 0.38532  Acc: 0.8421875  Time: 13.77  \n",
      "[E3 B2000]  Loss: 0.39766  Acc: 0.8321875  Time: 13.76  \n",
      "[E3 B2100]  Loss: 0.38315  Acc: 0.8346875  Time: 13.77  \n",
      "[Epoch 3] 322.42 seconds | Train acc: 0.8335 Train loss: 0.3828 Val acc: 0.8349 Val loss: 0.3788\n",
      "--- Epoch: 4 ---\n",
      "[E4 B100]  Loss: 0.38994  Acc: 0.8346875  Time: 13.76  \n",
      "[E4 B200]  Loss: 0.38969  Acc: 0.82875  Time: 13.76  \n",
      "[E4 B300]  Loss: 0.39195  Acc: 0.8296875  Time: 13.75  \n",
      "[E4 B400]  Loss: 0.38438  Acc: 0.838125  Time: 13.76  \n",
      "[E4 B500]  Loss: 0.38867  Acc: 0.82375  Time: 13.76  \n",
      "[E4 B600]  Loss: 0.38053  Acc: 0.84  Time: 13.76  \n",
      "[E4 B700]  Loss: 0.37715  Acc: 0.83625  Time: 13.76  \n",
      "[E4 B800]  Loss: 0.39120  Acc: 0.833125  Time: 13.77  \n",
      "[E4 B900]  Loss: 0.37983  Acc: 0.835625  Time: 13.77  \n",
      "[E4 B1000]  Loss: 0.37878  Acc: 0.836875  Time: 13.77  \n",
      "[E4 B1100]  Loss: 0.38343  Acc: 0.835625  Time: 13.77  \n",
      "[E4 B1200]  Loss: 0.38141  Acc: 0.8325  Time: 13.78  \n",
      "[E4 B1300]  Loss: 0.37506  Acc: 0.8296875  Time: 13.77  \n",
      "[E4 B1400]  Loss: 0.37331  Acc: 0.835625  Time: 13.77  \n",
      "[E4 B1500]  Loss: 0.37615  Acc: 0.8371875  Time: 13.77  \n",
      "[E4 B1600]  Loss: 0.37724  Acc: 0.836875  Time: 13.77  \n",
      "[E4 B1700]  Loss: 0.36903  Acc: 0.83625  Time: 13.77  \n",
      "[E4 B1800]  Loss: 0.36493  Acc: 0.8396875  Time: 13.76  \n",
      "[E4 B1900]  Loss: 0.39153  Acc: 0.8315625  Time: 13.77  \n",
      "[E4 B2000]  Loss: 0.38844  Acc: 0.831875  Time: 13.77  \n",
      "[E4 B2100]  Loss: 0.38405  Acc: 0.8296875  Time: 13.76  \n"
     ]
    }
   ],
   "source": [
    "trainer.train_loop(train_loader, val_loader, batch_logging=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 1 ---\n",
      "[CLS] i found myself growing more and more frustrated and detached as vincent became more and more abhorrent. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 2 ---\n",
      "[CLS] can get your money back [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 3 ---\n",
      "[CLS] to the climactic burst of violence [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 4 ---\n",
      "[CLS] an overripe episode of tv's dawson's creek [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "--- Sample 5 ---\n",
      "[CLS] ultimately takes hold and grips hard. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([0, 0, 1, 0, 1])\n",
      "tensor([0, 0, 0, 0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample_loader = DataLoader(split_datasets['train'], batch_size=5, shuffle=True)\n",
    "for i in sample_loader:\n",
    "    input_ids, attn_masks, labels = i\n",
    "    decoded = tokenizer.batch_decode(input_ids)\n",
    "    for j, d in enumerate(decoded):\n",
    "        print(f'--- Sample {j+1} ---')\n",
    "        print(d)\n",
    "    print(labels)\n",
    "    input_ids = input_ids.to(trainer.device)\n",
    "    attn_masks = attn_masks.to(trainer.device)\n",
    "    labels = labels.to(trainer.device)\n",
    "    outputs = trainer.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "    probas = F.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probas, axis=1)\n",
    "    print(preds)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
