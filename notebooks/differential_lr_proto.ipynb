{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate GPU is available for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec 13 16:28:05 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0    36W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset):\n",
    "    ds = datasets.load_dataset('glue', dataset)\n",
    "    num_classes = ds['train'].features['label'].num_classes\n",
    "    return ds, num_classes\n",
    "\n",
    "\n",
    "def create_dataset_from_text_dataset(ds, tokenizer):\n",
    "    encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attn_masks = encoding['attention_mask']\n",
    "    labels = torch.tensor(ds['label'])\n",
    "    return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "\n",
    "def get_tensor_datasets(dataset_dict, splits, tokenizer):\n",
    "    split_datasets = {}\n",
    "    for s in splits:\n",
    "        split_datasets[s] = create_dataset_from_text_dataset(dataset_dict[s], tokenizer)\n",
    "    return split_datasets\n",
    "\n",
    "\n",
    "def get_data_loaders(split_datasets, batch_size):\n",
    "    train_loader = DataLoader(split_datasets['train'], batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(split_datasets['validation'], batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(split_datasets['test'], batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sst2 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = 'sst2'\n",
    "print(f'Loading {dataset} dataset...')\n",
    "dataset_dict, num_classes = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# sst2 = datasets.load_dataset('glue', 'sst2')\n",
    "# sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     print(sst2['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sst2['train'].features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AdapterType\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_transformer(model_name, num_labels, adapter, dataset):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        if adapter:\n",
    "            model.add_adapter(dataset, AdapterType.text_task)\n",
    "            model.train_adapter(dataset)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_criterion(num_labels):\n",
    "    if num_labels == 2:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for bert-base-uncased...\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "print(f'Loading tokenizer for {model_name}...')\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader for various splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loader for ['train', 'validation', 'test'] splits...\n"
     ]
    }
   ],
   "source": [
    "# Create data loader for each split\n",
    "splits = list(dataset_dict.keys())\n",
    "print(f'Creating data loader for {splits} splits...')\n",
    "split_datasets = get_tensor_datasets(dataset_dict, splits, tokenizer)\n",
    "train_loader, val_loader, test_loader = get_data_loaders(split_datasets, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7fe0b1658cd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_batch, sample_batched in enumerate(train_loader):\n",
    "#     print(i_batch, sample_batched[2].size())\n",
    "    \n",
    "#     if i_batch == 0:\n",
    "#         print(sample_batched[2])\n",
    "#         sb = sample_batched[2].to('cuda')\n",
    "#         print(sb)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# def create_dataset_from_text_dataset(ds):\n",
    "#     encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "#     input_ids = encoding['input_ids']\n",
    "#     attn_masks = encoding['attention_mask']\n",
    "#     labels = torch.tensor(ds['label'])\n",
    "    \n",
    "#     return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "# splits = ['train',  'validation', 'test']\n",
    "# split_datasets = {}\n",
    "\n",
    "# for s in splits:\n",
    "#     split_datasets[s] = create_dataset_from_text_dataset(sst2[s])\n",
    "\n",
    "# split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data loader\n",
    "# sample_loader = DataLoader(split_datasets['train'], batch_size=3, shuffle=True)\n",
    "# for i in sample_loader:\n",
    "#     input_ids, attn_masks, labels = i\n",
    "#     decoded = tokenizer.batch_decode(input_ids)\n",
    "#     for d in decoded:\n",
    "#         print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased with adapters=False...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "adapter = False\n",
    "print(f'Loading {model_name} with adapters={adapter}...')\n",
    "model = get_transformer(model_name,\n",
    "                        num_labels=num_classes,\n",
    "                        adapter=adapter,\n",
    "                        dataset=dataset)\n",
    "criterion = get_criterion(num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, child in model.named_children():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(add_adapters=False):\n",
    "#     model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "#     if add_adapters:\n",
    "#         from transformers import AdapterType\n",
    "#         model.add_adapter(\"sst2\", AdapterType.text_task)\n",
    "#         model.train_adapter(\"sst2\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(split_datasets['train'], batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(split_datasets['validation'], batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "# logging.set_verbosity_warning()\n",
    "\n",
    "# model = create_model(add_adapters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Learning Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_learning_scheme(learning_scheme, model, learning_rate, adapter, epoch):\n",
    "    if learning_scheme == 'differential':\n",
    "        optimizer_grouped_parameters = differential_learning_scheme(model, learning_rate)\n",
    "        optimizer = torch.optim.SGD(optimizer_grouped_parameters)\n",
    "    elif learning_scheme == 'fixed':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif learning_scheme == 'nesterov':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "    elif learning_scheme == 'gradual-unfreeze':\n",
    "        optimizer_grouped_parameters = gradual_unfreezing_learning_scheme(model, learning_rate, adapter, epoch)\n",
    "        optimizer = torch.optim.SGD(optimizer_grouped_parameters)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def differential_learning_scheme(model, learning_rate=0.1, divisor=2.6):\n",
    "    param_prefixes = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "            if base not in param_prefixes:\n",
    "                param_prefixes[base] = 0\n",
    "\n",
    "    param_prefix_divisors = list(reversed([divisor * i for i in range(1, len(param_prefixes))])) + [1]\n",
    "    param_learning_rates = [learning_rate / ld for ld in param_prefix_divisors]\n",
    "\n",
    "    param_prefix_lr_lookup = dict(zip(param_prefixes.keys(), param_learning_rates))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': p, 'lr': param_prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "        for n, p in model.named_parameters() if p.requires_grad\n",
    "    ]\n",
    "\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def gradual_unfreezing_learning_scheme(model, learning_rate, adapter, epoch=1):\n",
    "    trainable_layers = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "            if adapter:\n",
    "                if base not in trainable_layers and 'adapter' or 'classifier' in base:\n",
    "                    trainable_layers.append(base)\n",
    "            else:\n",
    "                if base not in trainable_layers:\n",
    "                    trainable_layers.append(base)\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': p, 'lr': learning_rate}\n",
    "        for n, p in model.named_parameters() if p.requires_grad and n.partition('.weight')[0].partition('.bias')[0] in trainable_layers[-epoch:]\n",
    "    ]\n",
    "\n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "\n",
    "def get_scheduler(scheduler, optimizer, learning_rate, max_lr):\n",
    "    if scheduler:\n",
    "        if scheduler == 'cyclic-triangular':\n",
    "            scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,\n",
    "                                                          base_lr=learning_rate,\n",
    "                                                          max_lr=max_lr,\n",
    "                                                          mode='triangular')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring fixed learning scheme...\n"
     ]
    }
   ],
   "source": [
    "# Get learning scheme\n",
    "learning_scheme = 'fixed'\n",
    "print(f'Configuring {learning_scheme} learning scheme...')\n",
    "optimizer = get_learning_scheme(learning_scheme, model, learning_rate=0.01, adapter=adapter, epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefixes = {}\n",
    "# for n, p in model.named_parameters():\n",
    "#     base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "#     if base not in prefixes:\n",
    "#         prefixes[base] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_LR = 0.1\n",
    "# BASE_DIVISOR = 2.6\n",
    "\n",
    "# prefix_divisors = list(reversed([BASE_DIVISOR * i for i in range(1, len(prefixes))])) + [1]\n",
    "# layer_learning_rates = [BASE_LR / ld for ld in prefix_divisors]\n",
    "\n",
    "# prefix_lr_lookup = dict(zip(prefixes.keys(), layer_learning_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': p, 'lr': prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "#     for n, p in model.named_parameters()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('Using device:', device)\n",
    "model = model.to(device)\n",
    "n_epochs = 10\n",
    "optimizer = optimizer\n",
    "scheduler = None\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_MINI_BATCH_CHECK = 5\n",
    "\n",
    "def measure_performance(loader):\n",
    "    running_loss = 0.0\n",
    "    correct_count = 0.0\n",
    "    total_count = 0.0\n",
    "    for data in loader:\n",
    "        input_ids = data[0].to(device)\n",
    "        attn_masks = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            probas = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probas, axis=1)\n",
    "\n",
    "            # Track stats\n",
    "            running_loss += loss\n",
    "            correct_count += torch.sum(preds == labels)\n",
    "            total_count += len(labels)\n",
    "\n",
    "    running_loss /= len(loader)\n",
    "    acc = correct_count / total_count\n",
    "\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  101, 15491, 28616,  ...,     0,     0,     0],\n",
      "        [  101,  2023,  2143,  ...,     0,     0,     0],\n",
      "        [  101,  2011,  1996,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  2182,  1005,  ...,     0,     0,     0],\n",
      "        [  101,  2087,  2047,  ...,     0,     0,     0],\n",
      "        [  101,  2061,  2261,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test_loader):\n",
    "    print(data)\n",
    "    print(len(data))\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6890fa73a0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeasure_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-ebef029f55c0>\u001b[0m in \u001b[0;36mmeasure_performance\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtotal_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mattn_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "measure_performance(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_loader:\n",
    "    print('Initial evaluating on validation dataset')\n",
    "    val_loss, val_acc = measure_performance(val_loader)\n",
    "    epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "    print(epoch_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f'--- Epoch: {epoch} ---')\n",
    "    epoch_start_time = time.time()\n",
    "    batch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input_ids = data[0].to(device)\n",
    "        attn_masks = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Evaluation/optimization step\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Print statistics periodically\n",
    "        running_loss += loss.item()\n",
    "        if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "            batch_end_time = time.time()\n",
    "            total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "            print(\n",
    "                f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "                f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "                f'Time: {total_batch_time:.2f} ',\n",
    "                f'LR: {scheduler.get_last_lr()}' if scheduler else '')\n",
    "\n",
    "            # Reset statistics\n",
    "            batch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    total_epoch_time = epoch_end_time - epoch_start_time\n",
    "    epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "\n",
    "    if val_loader:\n",
    "        val_loss, val_acc = measure_performance(val_loader)\n",
    "        epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {val_loss:.4f}'\n",
    "\n",
    "    print(epoch_summary)\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import torch.nn.functional as F\n",
    "# import copy\n",
    "\n",
    "# class Trainer:\n",
    "#     def __init__(self, model, n_epochs, optimizer):\n",
    "#         self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.model = model.to(self.device)\n",
    "#         self.n_epochs = n_epochs\n",
    "#         self.optimizer = optimizer\n",
    "#         self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "# #         no_decay = ['bias', 'LayerNorm.weight']\n",
    "# #         optimizer_grouped_parameters = [\n",
    "# #             {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "# #             {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# #         ]\n",
    "# #         self.optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=1e-3, momentum=0.9)\n",
    "#         # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer)\n",
    "#         # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10000, T_mult=2)        \n",
    "\n",
    "#     def measure_performance(self, loader):\n",
    "#         running_loss = 0.0\n",
    "#         correct_count = 0\n",
    "#         total_count = 0\n",
    "#         for data in loader:\n",
    "#             input_ids = data[0].to(self.device)\n",
    "#             attn_masks = data[1].to(self.device)\n",
    "#             labels = data[2].to(self.device)\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#                 loss = self.criterion(outputs, labels)\n",
    "#                 probas = F.softmax(outputs, dim=1)\n",
    "#                 preds = torch.argmax(probas, axis=1)\n",
    "                \n",
    "#                 # Track stats\n",
    "#                 running_loss += loss\n",
    "#                 correct_count += torch.sum(preds == labels) \n",
    "#                 total_count += len(labels) \n",
    "        \n",
    "#         running_loss /= len(loader)\n",
    "#         acc = correct_count / total_count\n",
    "\n",
    "#         return running_loss, acc\n",
    "\n",
    "#     def train_loop(self, train_loader, val_loader=None):\n",
    "#         print('Starting training loop\\n\\n')\n",
    "#         N_MINI_BATCH_CHECK = 200\n",
    "\n",
    "#         if val_loader:\n",
    "#             print('Initial evaluating on validation dataset')\n",
    "#             val_loss, val_acc = self.measure_performance(val_loader)\n",
    "#             epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "#             print(epoch_summary)\n",
    "\n",
    "#         for epoch in range(self.n_epochs):\n",
    "#             print(f'--- Epoch: {epoch} ---')\n",
    "#             epoch_start_time = time.time()\n",
    "#             batch_start_time = time.time()\n",
    "#             running_loss = 0.0\n",
    "\n",
    "#             for i, data in enumerate(train_loader):\n",
    "#                 input_ids = data[0].to(self.device)\n",
    "#                 attn_masks = data[1].to(self.device)\n",
    "#                 labels = data[2].to(self.device)\n",
    "\n",
    "#                 self.optimizer.zero_grad()\n",
    "\n",
    "#                 # Evaluation/optimization step\n",
    "#                 outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#                 loss = self.criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 if self.scheduler:\n",
    "#                     self.scheduler.step()\n",
    "                \n",
    "#                 # Print statistics periodically\n",
    "#                 running_loss += loss.item()\n",
    "#                 if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "#                     batch_end_time = time.time()\n",
    "#                     total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "#                     print(\n",
    "#                         f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "#                         f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "#                         f'Time: {total_batch_time:.2f} ',\n",
    "#                         f'LR: {self.scheduler.get_last_lr()}' if self.scheduler else '')\n",
    "\n",
    "#                     # Reset statistics\n",
    "#                     batch_start_time = time.time()\n",
    "#                     running_loss = 0.0\n",
    "\n",
    "#             epoch_end_time = time.time()\n",
    "#             total_epoch_time = epoch_end_time - epoch_start_time\n",
    "#             epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "            \n",
    "#             if val_loader:\n",
    "#                 val_loss, val_acc = self.measure_performance(val_loader)\n",
    "#                 epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {loss:.4f}'\n",
    "\n",
    "#             print(epoch_summary)\n",
    "\n",
    "#         print('Finished training')\n",
    "\n",
    "#     def lr_test(self, train_loader, lrs=(-9, 2)):\n",
    "#         \"\"\"\n",
    "#         lrs = (min_lr, max_lr, factor_scale)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Prepare LR-finder loop\n",
    "#         model = copy.deepcopy(self.model).to(self.device)\n",
    "#         min_lr, max_lr = lrs\n",
    "#         lrs = np.logspace(min_lr, max_lr, num=len(train_loader), endpoint=True)\n",
    "#         losses = []\n",
    "#         for i, data in enumerate(iter(train_loader)):\n",
    "#             curr_lr = lrs[i]\n",
    "#             optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr)\n",
    "\n",
    "#             input_ids = data[0].to(self.device)\n",
    "#             attn_masks = data[1].to(self.device)\n",
    "#             labels = data[2].to(self.device)\n",
    "\n",
    "#             # Evaluation/optimization step\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#             loss = self.criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             losses.append(loss)\n",
    "\n",
    "#             if i % 100 == 0:\n",
    "#                 print(f'Step [{i}, {len(train_loader)}] | LR: {curr_lr:.4e} | Loss: {loss:.4f}')\n",
    "\n",
    "#         return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, 10, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train_loop(train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu101.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
