{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate GPU is available for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  9 20:04:37 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   63C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset):\n",
    "    ds = datasets.load_dataset('glue', dataset)\n",
    "    num_classes = ds['train'].features['label'].num_classes\n",
    "    return ds, num_classes\n",
    "\n",
    "\n",
    "def create_dataset_from_text_dataset(ds, tokenizer):\n",
    "    encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attn_masks = encoding['attention_mask']\n",
    "    labels = torch.tensor(ds['label'])\n",
    "    return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "\n",
    "def get_tensor_datasets(dataset_dict, splits, tokenizer):\n",
    "    split_datasets = {}\n",
    "    for s in splits:\n",
    "        split_datasets[s] = create_dataset_from_text_dataset(dataset_dict[s], tokenizer)\n",
    "    return split_datasets\n",
    "\n",
    "\n",
    "def get_data_loaders(split_datasets):\n",
    "    train_loader = DataLoader(split_datasets['train'], batch_size=256, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(split_datasets['validation'], batch_size=256, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sst2 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = 'sst2'\n",
    "print(f'Loading {dataset} dataset...')\n",
    "dataset_dict, num_classes = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# sst2 = datasets.load_dataset('glue', 'sst2')\n",
    "# sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     print(sst2['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sst2['train'].features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AdapterType\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_transformer(model_name, num_labels, adapter, dataset):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        if adapter:\n",
    "            model.add_adapter(dataset, AdapterType.text_task)\n",
    "            model.train_adapter(dataset)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_criterion(num_labels):\n",
    "    if num_labels == 2:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for bert-base-uncased...\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "print(f'Loading tokenizer for {model_name}...')\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader for various splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data loader for ['train', 'validation', 'test'] splits...\n"
     ]
    }
   ],
   "source": [
    "# Create data loader for each split\n",
    "splits = list(dataset_dict.keys())\n",
    "print(f'Creating data loader for {splits} splits...')\n",
    "split_datasets = get_tensor_datasets(dataset_dict, splits, tokenizer)\n",
    "train_loader, val_loader = get_data_loaders(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([256])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched[2].size())\n",
    "    \n",
    "    if i_batch == 0:\n",
    "        print(sample_batched[2])\n",
    "        sb = sample_batched[2].to('cuda')\n",
    "        print(sb)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# def create_dataset_from_text_dataset(ds):\n",
    "#     encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "#     input_ids = encoding['input_ids']\n",
    "#     attn_masks = encoding['attention_mask']\n",
    "#     labels = torch.tensor(ds['label'])\n",
    "    \n",
    "#     return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "# splits = ['train',  'validation', 'test']\n",
    "# split_datasets = {}\n",
    "\n",
    "# for s in splits:\n",
    "#     split_datasets[s] = create_dataset_from_text_dataset(sst2[s])\n",
    "\n",
    "# split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data loader\n",
    "# sample_loader = DataLoader(split_datasets['train'], batch_size=3, shuffle=True)\n",
    "# for i in sample_loader:\n",
    "#     input_ids, attn_masks, labels = i\n",
    "#     decoded = tokenizer.batch_decode(input_ids)\n",
    "#     for d in decoded:\n",
    "#         print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-uncased with adapters=True...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "adapter = True\n",
    "print(f'Loading {model_name} with adapters={adapter}...')\n",
    "model = get_transformer(model_name,\n",
    "                        num_labels=num_classes,\n",
    "                        adapter=adapter,\n",
    "                        dataset=dataset)\n",
    "criterion = get_criterion(num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(add_adapters=False):\n",
    "#     model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "#     if add_adapters:\n",
    "#         from transformers import AdapterType\n",
    "#         model.add_adapter(\"sst2\", AdapterType.text_task)\n",
    "#         model.train_adapter(\"sst2\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(split_datasets['train'], batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(split_datasets['validation'], batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "# logging.set_verbosity_warning()\n",
    "\n",
    "# model = create_model(add_adapters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Learning Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_learning_scheme(learning_scheme, model):\n",
    "    if learning_scheme == 'differential':\n",
    "        optimizer_grouped_parameters = differential_learning_scheme(model)\n",
    "        optimizer = torch.optim.SGD(optimizer_grouped_parameters)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def differential_learning_scheme(model, learning_rate=0.1, divisor=2.6):\n",
    "    param_prefixes = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "        if base not in param_prefixes:\n",
    "            param_prefixes[base] = 0\n",
    "\n",
    "    param_prefix_divisors = list(reversed([divisor * i for i in range(1, len(param_prefixes))])) + [1]\n",
    "    param_learning_rates = [learning_rate / ld for ld in param_prefix_divisors]\n",
    "\n",
    "    param_prefix_lr_lookup = dict(zip(param_prefixes.keys(), param_learning_rates))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': p, 'lr': param_prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "        for n, p in model.named_parameters()\n",
    "    ]\n",
    "\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring differential learning scheme...\n"
     ]
    }
   ],
   "source": [
    "# Get learning scheme\n",
    "learning_scheme = 'differential'\n",
    "print(f'Configuring {learning_scheme} learning scheme...')\n",
    "optimizer = get_learning_scheme(learning_scheme, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefixes = {}\n",
    "# for n, p in model.named_parameters():\n",
    "#     base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "#     if base not in prefixes:\n",
    "#         prefixes[base] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_LR = 0.1\n",
    "# BASE_DIVISOR = 2.6\n",
    "\n",
    "# prefix_divisors = list(reversed([BASE_DIVISOR * i for i in range(1, len(prefixes))])) + [1]\n",
    "# layer_learning_rates = [BASE_LR / ld for ld in prefix_divisors]\n",
    "\n",
    "# prefix_lr_lookup = dict(zip(prefixes.keys(), layer_learning_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': p, 'lr': prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "#     for n, p in model.named_parameters()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('Using device:', device)\n",
    "model = model.to(device)\n",
    "n_epochs = 10\n",
    "optimizer = optimizer\n",
    "scheduler = None\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_MINI_BATCH_CHECK = 5\n",
    "\n",
    "def measure_performance(loader):\n",
    "    running_loss = 0.0\n",
    "    correct_count = 0.0\n",
    "    total_count = 0.0\n",
    "    for data in loader:\n",
    "        input_ids = data[0].to(device)\n",
    "        attn_masks = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            probas = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probas, axis=1)\n",
    "\n",
    "            # Track stats\n",
    "            running_loss += loss\n",
    "            correct_count += torch.sum(preds == labels)\n",
    "            total_count += len(labels)\n",
    "\n",
    "    running_loss /= len(loader)\n",
    "    acc = correct_count / total_count\n",
    "\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial evaluating on validation dataset\n",
      "[Epoch 0] | Val acc: 0.5069 Val loss: 0.6960\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if val_loader:\n",
    "    print('Initial evaluating on validation dataset')\n",
    "    val_loss, val_acc = measure_performance(val_loader)\n",
    "    epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "    print(epoch_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch: 0 ---\n",
      "[E1 B5]  Loss: 2.66172  Time: 4.58  \n",
      "[E1 B10]  Loss: 3.89834  Time: 4.57  \n",
      "[E1 B15]  Loss: 2.60903  Time: 4.58  \n",
      "[E1 B20]  Loss: 3.19631  Time: 4.57  \n",
      "[E1 B25]  Loss: 2.63111  Time: 4.57  \n",
      "[E1 B30]  Loss: 2.24607  Time: 4.57  \n",
      "[E1 B35]  Loss: 2.66180  Time: 4.57  \n",
      "[E1 B40]  Loss: 2.24799  Time: 4.57  \n",
      "[E1 B45]  Loss: 2.78952  Time: 4.57  \n",
      "[E1 B50]  Loss: 2.01484  Time: 4.57  \n",
      "[E1 B55]  Loss: 1.98343  Time: 4.57  \n",
      "[E1 B60]  Loss: 2.23517  Time: 4.57  \n",
      "[E1 B65]  Loss: 2.16794  Time: 4.57  \n",
      "[E1 B70]  Loss: 2.15105  Time: 4.57  \n",
      "[E1 B75]  Loss: 2.17239  Time: 4.58  \n",
      "[E1 B80]  Loss: 1.86547  Time: 4.57  \n",
      "[E1 B85]  Loss: 1.65133  Time: 4.57  \n",
      "[E1 B90]  Loss: 1.52771  Time: 4.57  \n",
      "[E1 B95]  Loss: 1.94157  Time: 4.57  \n",
      "[E1 B100]  Loss: 1.84665  Time: 4.65  \n",
      "[E1 B105]  Loss: 1.78651  Time: 4.57  \n",
      "[E1 B110]  Loss: 1.46416  Time: 4.57  \n",
      "[E1 B115]  Loss: 1.08384  Time: 4.57  \n",
      "[E1 B120]  Loss: 1.73150  Time: 4.57  \n",
      "[E1 B125]  Loss: 1.20199  Time: 4.57  \n",
      "[E1 B130]  Loss: 1.22776  Time: 4.57  \n",
      "[E1 B135]  Loss: 1.05448  Time: 4.58  \n",
      "[E1 B140]  Loss: 1.26629  Time: 4.57  \n",
      "[E1 B145]  Loss: 1.08061  Time: 4.57  \n",
      "[E1 B150]  Loss: 1.23391  Time: 4.57  \n",
      "[E1 B155]  Loss: 1.12712  Time: 4.57  \n",
      "[E1 B160]  Loss: 0.96048  Time: 4.58  \n",
      "[E1 B165]  Loss: 0.99241  Time: 4.57  \n",
      "[E1 B170]  Loss: 0.88369  Time: 4.57  \n",
      "[E1 B175]  Loss: 0.98464  Time: 4.57  \n",
      "[E1 B180]  Loss: 0.83492  Time: 4.57  \n",
      "[E1 B185]  Loss: 0.92586  Time: 4.57  \n",
      "[E1 B190]  Loss: 0.78986  Time: 4.58  \n",
      "[E1 B195]  Loss: 0.80332  Time: 4.57  \n",
      "[E1 B200]  Loss: 0.81082  Time: 4.58  \n",
      "[E1 B205]  Loss: 0.85918  Time: 4.57  \n",
      "[E1 B210]  Loss: 0.87271  Time: 4.57  \n",
      "[E1 B215]  Loss: 0.77769  Time: 4.57  \n",
      "[E1 B220]  Loss: 0.74503  Time: 4.58  \n",
      "[E1 B225]  Loss: 0.74589  Time: 4.57  \n",
      "[E1 B230]  Loss: 0.75074  Time: 4.57  \n",
      "[E1 B235]  Loss: 0.80767  Time: 4.57  \n",
      "[E1 B240]  Loss: 0.71627  Time: 4.57  \n",
      "[E1 B245]  Loss: 0.69173  Time: 4.57  \n",
      "[E1 B250]  Loss: 0.74926  Time: 4.57  \n",
      "[E1 B255]  Loss: 0.71840  Time: 4.57  \n",
      "[E1 B260]  Loss: 0.76538  Time: 4.57  \n",
      "[Epoch 1] 240.87908816337585 seconds | Val acc: 0.5161 | Val loss: 2.8352\n",
      "--- Epoch: 1 ---\n",
      "[E2 B5]  Loss: 0.91275  Time: 4.58  \n",
      "[E2 B10]  Loss: 0.70331  Time: 4.57  \n",
      "[E2 B15]  Loss: 0.70804  Time: 4.58  \n",
      "[E2 B20]  Loss: 0.74053  Time: 4.57  \n",
      "[E2 B25]  Loss: 0.71461  Time: 4.58  \n",
      "[E2 B30]  Loss: 0.79047  Time: 4.57  \n",
      "[E2 B35]  Loss: 0.72703  Time: 4.57  \n",
      "[E2 B40]  Loss: 0.73746  Time: 4.57  \n",
      "[E2 B45]  Loss: 0.71315  Time: 4.57  \n",
      "[E2 B50]  Loss: 0.70136  Time: 4.57  \n",
      "[E2 B55]  Loss: 0.71501  Time: 4.58  \n",
      "[E2 B60]  Loss: 0.78430  Time: 4.57  \n",
      "[E2 B65]  Loss: 0.71720  Time: 4.58  \n",
      "[E2 B70]  Loss: 0.73144  Time: 4.58  \n",
      "[E2 B75]  Loss: 0.71564  Time: 4.57  \n",
      "[E2 B80]  Loss: 0.72987  Time: 4.57  \n",
      "[E2 B85]  Loss: 0.75585  Time: 4.57  \n",
      "[E2 B90]  Loss: 0.78256  Time: 4.57  \n",
      "[E2 B95]  Loss: 0.74923  Time: 4.58  \n",
      "[E2 B100]  Loss: 0.74776  Time: 4.57  \n",
      "[E2 B105]  Loss: 0.70842  Time: 4.57  \n",
      "[E2 B110]  Loss: 0.72688  Time: 4.57  \n",
      "[E2 B115]  Loss: 0.69875  Time: 4.57  \n",
      "[E2 B120]  Loss: 0.72278  Time: 4.57  \n",
      "[E2 B125]  Loss: 0.69985  Time: 4.57  \n",
      "[E2 B130]  Loss: 0.72359  Time: 4.58  \n",
      "[E2 B135]  Loss: 0.71498  Time: 4.58  \n",
      "[E2 B140]  Loss: 0.68655  Time: 4.57  \n",
      "[E2 B145]  Loss: 0.69980  Time: 4.58  \n",
      "[E2 B150]  Loss: 0.76106  Time: 4.57  \n",
      "[E2 B155]  Loss: 0.69965  Time: 4.58  \n",
      "[E2 B160]  Loss: 0.71487  Time: 4.57  \n",
      "[E2 B165]  Loss: 0.73636  Time: 4.58  \n",
      "[E2 B170]  Loss: 0.72422  Time: 4.57  \n",
      "[E2 B175]  Loss: 0.77595  Time: 4.58  \n",
      "[E2 B180]  Loss: 0.72570  Time: 4.57  \n",
      "[E2 B185]  Loss: 0.70027  Time: 4.57  \n",
      "[E2 B190]  Loss: 0.75245  Time: 4.57  \n",
      "[E2 B195]  Loss: 0.79757  Time: 4.57  \n",
      "[E2 B200]  Loss: 0.69923  Time: 4.58  \n",
      "[E2 B205]  Loss: 0.70446  Time: 4.57  \n",
      "[E2 B210]  Loss: 0.70899  Time: 4.57  \n",
      "[E2 B215]  Loss: 0.69405  Time: 4.58  \n",
      "[E2 B220]  Loss: 0.70599  Time: 4.57  \n",
      "[E2 B225]  Loss: 0.77498  Time: 4.58  \n",
      "[E2 B230]  Loss: 0.72330  Time: 4.58  \n",
      "[E2 B235]  Loss: 0.74243  Time: 4.57  \n",
      "[E2 B240]  Loss: 0.81767  Time: 4.58  \n",
      "[E2 B245]  Loss: 0.70448  Time: 4.58  \n",
      "[E2 B250]  Loss: 0.70849  Time: 4.57  \n",
      "[E2 B255]  Loss: 0.69293  Time: 4.57  \n",
      "[E2 B260]  Loss: 0.69914  Time: 4.57  \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f'--- Epoch: {epoch} ---')\n",
    "    epoch_start_time = time.time()\n",
    "    batch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input_ids = data[0].to(device)\n",
    "        attn_masks = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Evaluation/optimization step\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Print statistics periodically\n",
    "        running_loss += loss.item()\n",
    "        if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "            batch_end_time = time.time()\n",
    "            total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "            print(\n",
    "                f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "                f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "                f'Time: {total_batch_time:.2f} ',\n",
    "                f'LR: {scheduler.get_last_lr()}' if scheduler else '')\n",
    "\n",
    "            # Reset statistics\n",
    "            batch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    total_epoch_time = epoch_end_time - epoch_start_time\n",
    "    epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "\n",
    "    if val_loader:\n",
    "        val_loss, val_acc = measure_performance(val_loader)\n",
    "        epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {val_loss:.4f}'\n",
    "\n",
    "    print(epoch_summary)\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import torch.nn.functional as F\n",
    "# import copy\n",
    "\n",
    "# class Trainer:\n",
    "#     def __init__(self, model, n_epochs, optimizer):\n",
    "#         self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.model = model.to(self.device)\n",
    "#         self.n_epochs = n_epochs\n",
    "#         self.optimizer = optimizer\n",
    "#         self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "# #         no_decay = ['bias', 'LayerNorm.weight']\n",
    "# #         optimizer_grouped_parameters = [\n",
    "# #             {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "# #             {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# #         ]\n",
    "# #         self.optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=1e-3, momentum=0.9)\n",
    "#         # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer)\n",
    "#         # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10000, T_mult=2)        \n",
    "\n",
    "#     def measure_performance(self, loader):\n",
    "#         running_loss = 0.0\n",
    "#         correct_count = 0\n",
    "#         total_count = 0\n",
    "#         for data in loader:\n",
    "#             input_ids = data[0].to(self.device)\n",
    "#             attn_masks = data[1].to(self.device)\n",
    "#             labels = data[2].to(self.device)\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#                 loss = self.criterion(outputs, labels)\n",
    "#                 probas = F.softmax(outputs, dim=1)\n",
    "#                 preds = torch.argmax(probas, axis=1)\n",
    "                \n",
    "#                 # Track stats\n",
    "#                 running_loss += loss\n",
    "#                 correct_count += torch.sum(preds == labels) \n",
    "#                 total_count += len(labels) \n",
    "        \n",
    "#         running_loss /= len(loader)\n",
    "#         acc = correct_count / total_count\n",
    "\n",
    "#         return running_loss, acc\n",
    "\n",
    "#     def train_loop(self, train_loader, val_loader=None):\n",
    "#         print('Starting training loop\\n\\n')\n",
    "#         N_MINI_BATCH_CHECK = 200\n",
    "\n",
    "#         if val_loader:\n",
    "#             print('Initial evaluating on validation dataset')\n",
    "#             val_loss, val_acc = self.measure_performance(val_loader)\n",
    "#             epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "#             print(epoch_summary)\n",
    "\n",
    "#         for epoch in range(self.n_epochs):\n",
    "#             print(f'--- Epoch: {epoch} ---')\n",
    "#             epoch_start_time = time.time()\n",
    "#             batch_start_time = time.time()\n",
    "#             running_loss = 0.0\n",
    "\n",
    "#             for i, data in enumerate(train_loader):\n",
    "#                 input_ids = data[0].to(self.device)\n",
    "#                 attn_masks = data[1].to(self.device)\n",
    "#                 labels = data[2].to(self.device)\n",
    "\n",
    "#                 self.optimizer.zero_grad()\n",
    "\n",
    "#                 # Evaluation/optimization step\n",
    "#                 outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#                 loss = self.criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 if self.scheduler:\n",
    "#                     self.scheduler.step()\n",
    "                \n",
    "#                 # Print statistics periodically\n",
    "#                 running_loss += loss.item()\n",
    "#                 if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "#                     batch_end_time = time.time()\n",
    "#                     total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "#                     print(\n",
    "#                         f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "#                         f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "#                         f'Time: {total_batch_time:.2f} ',\n",
    "#                         f'LR: {self.scheduler.get_last_lr()}' if self.scheduler else '')\n",
    "\n",
    "#                     # Reset statistics\n",
    "#                     batch_start_time = time.time()\n",
    "#                     running_loss = 0.0\n",
    "\n",
    "#             epoch_end_time = time.time()\n",
    "#             total_epoch_time = epoch_end_time - epoch_start_time\n",
    "#             epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "            \n",
    "#             if val_loader:\n",
    "#                 val_loss, val_acc = self.measure_performance(val_loader)\n",
    "#                 epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {loss:.4f}'\n",
    "\n",
    "#             print(epoch_summary)\n",
    "\n",
    "#         print('Finished training')\n",
    "\n",
    "#     def lr_test(self, train_loader, lrs=(-9, 2)):\n",
    "#         \"\"\"\n",
    "#         lrs = (min_lr, max_lr, factor_scale)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Prepare LR-finder loop\n",
    "#         model = copy.deepcopy(self.model).to(self.device)\n",
    "#         min_lr, max_lr = lrs\n",
    "#         lrs = np.logspace(min_lr, max_lr, num=len(train_loader), endpoint=True)\n",
    "#         losses = []\n",
    "#         for i, data in enumerate(iter(train_loader)):\n",
    "#             curr_lr = lrs[i]\n",
    "#             optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr)\n",
    "\n",
    "#             input_ids = data[0].to(self.device)\n",
    "#             attn_masks = data[1].to(self.device)\n",
    "#             labels = data[2].to(self.device)\n",
    "\n",
    "#             # Evaluation/optimization step\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#             loss = self.criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             losses.append(loss)\n",
    "\n",
    "#             if i % 100 == 0:\n",
    "#                 print(f'Step [{i}, {len(train_loader)}] | LR: {curr_lr:.4e} | Loss: {loss:.4f}')\n",
    "\n",
    "#         return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, 10, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train_loop(train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
