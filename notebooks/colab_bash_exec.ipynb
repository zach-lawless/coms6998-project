{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6MHb3CsX60-u",
    "outputId": "2e77a2f6-e54d-4170-c25a-8892fe04634e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User name: zach-lawless\n",
      "Password: ··········\n",
      "Repo name: zach-lawless/coms6998-project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "\n",
    "user = input('User name: ')\n",
    "password = getpass('Password: ')\n",
    "password = urllib.parse.quote(password) # your password is converted into url format\n",
    "repo_name = input('Repo name: ')\n",
    "\n",
    "cmd_string = f'git clone https://{user}:{password}@github.com/{repo_name}.git'\n",
    "\n",
    "os.system(cmd_string)\n",
    "cmd_string, password = \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsi7cjTW8cs4",
    "outputId": "e0ee9116-650d-4b5e-b010-aaae60b131e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 11 22:47:32 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   59C    P8    13W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZk1j0Hn7rN9",
    "outputId": "d7313308-e84e-4c7a-de23-08139a1b4c76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
      "Collecting adapter-transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/9d/44/1370c187aba1349d56d6813ec4de54644d15e154983050f4923ce5455069/adapter_transformers-1.1.0-py3-none-any.whl (1.3MB)\n",
      "Collecting xxhash\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.70.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (1.18.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (2.23.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (1.1.5)\n",
      "Collecting pyarrow>=0.17.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (4.41.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.8)\n",
      "Collecting sacremoses\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "Collecting sentencepiece==0.1.91\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (20.7)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (3.0.12)\n",
      "Collecting tokenizers==0.9.3\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets->-r coms6998-project/requirements.txt (line 1)) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets->-r coms6998-project/requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (50.3.2)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0c6bb4b2ca504f9e48caa40a65b1bcf0f5351fcc4844aa63e54ee7582986b226\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: xxhash, pyarrow, datasets, sacremoses, sentencepiece, tokenizers, adapter-transformers\n",
      "  Found existing installation: pyarrow 0.14.1\n",
      "    Uninstalling pyarrow-0.14.1:\n",
      "      Successfully uninstalled pyarrow-0.14.1\n",
      "Successfully installed adapter-transformers-1.1.0 datasets-1.1.3 pyarrow-2.0.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 xxhash-2.0.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install -r coms6998-project/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_BcPELe7kAG",
    "outputId": "2e5d31f3-1014-4403-c96b-4b7b340655ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: run_trial.py [-h] [--name NAME] [--transformer {bert-base-uncased}]\n",
      "                    [--dataset {sst2}] [--batch-size BATCH_SIZE]\n",
      "                    [--batch-logging BATCH_LOGGING] [--adapter ADAPTER]\n",
      "                    [--learning-scheme {differential,fixed,nesterov}]\n",
      "                    [--learning-rate LEARNING_RATE]\n",
      "                    [--max_learning_rate MAX_LEARNING_RATE] [--epochs EPOCHS]\n",
      "                    [--scheduler {cyclic-triangular}]\n",
      "\n",
      "Run a finetuning trial\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --name NAME           name to give trial (for output saving purposes)\n",
      "  --transformer {bert-base-uncased}\n",
      "                        the specific transformer to finetune\n",
      "  --dataset {sst2}      the dataset you are finetuning on\n",
      "  --batch-size BATCH_SIZE\n",
      "                        how large of batches to feed to the transformer\n",
      "  --batch-logging BATCH_LOGGING\n",
      "                        how frequently to log and print finetuning batch info\n",
      "  --adapter ADAPTER     whether to add adapters to the transformer or not\n",
      "  --learning-scheme {differential,fixed,nesterov}\n",
      "                        the learning scheme to fine tune with\n",
      "  --learning-rate LEARNING_RATE\n",
      "                        the learning rate to use for finetuning\n",
      "  --max_learning_rate MAX_LEARNING_RATE\n",
      "                        the max learning rate if using a scheduler\n",
      "  --epochs EPOCHS       how many epochs to finetune for\n",
      "  --scheduler {cyclic-triangular}\n",
      "                        learning rate scheduler to use, if any\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-11 22:47:51.126046: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python coms6998-project/run_trial.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2HFBA4dy7Aqp",
    "outputId": "60f255b6-e00a-40b8-d9df-145d59e3dbe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sst2 dataset...\n",
      "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n",
      "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n",
      "Loading tokenizer for bert-base-uncased...\n",
      "Creating data loader for ['train', 'validation', 'test'] splits...\n",
      "Loading bert-base-uncased with adapters=None...\n",
      "Configuring fixed learning scheme...\n",
      "Setting up scheduler if any...\n",
      "Initializing Trainer object...\n",
      "Beginning finetuning...\n",
      "Starting training loop\n",
      "Initial evaluating on validation dataset\n",
      "[Epoch 0] | Train acc: 0.5578 Train loss: 0.7781 Val acc: 0.5092 Val loss: 0.8618\n",
      "--- Epoch: 1 ---\n",
      "[E1 B10]  Loss: 0.69451  Acc: 0.53359375  Time: 13.97  \n",
      "[E1 B20]  Loss: 0.66820  Acc: 0.56953125  Time: 13.97  \n",
      "[E1 B30]  Loss: 0.58556  Acc: 0.675  Time: 13.92  \n",
      "[E1 B40]  Loss: 0.48718  Acc: 0.76640625  Time: 13.86  \n",
      "[E1 B50]  Loss: 0.43754  Acc: 0.8109375  Time: 13.87  \n",
      "[E1 B60]  Loss: 0.42410  Acc: 0.82421875  Time: 13.91  \n",
      "[E1 B70]  Loss: 0.31838  Acc: 0.875  Time: 13.96  \n",
      "[E1 B80]  Loss: 0.38770  Acc: 0.83515625  Time: 13.95  \n",
      "[E1 B90]  Loss: 0.37710  Acc: 0.84453125  Time: 13.97  \n",
      "[E1 B100]  Loss: 0.30136  Acc: 0.8796875  Time: 13.98  \n",
      "[E1 B110]  Loss: 0.36502  Acc: 0.8546875  Time: 13.96  \n",
      "[E1 B120]  Loss: 0.29550  Acc: 0.88046875  Time: 13.97  \n",
      "[E1 B130]  Loss: 0.28010  Acc: 0.88359375  Time: 13.98  \n",
      "[E1 B140]  Loss: 0.26527  Acc: 0.89453125  Time: 13.96  \n",
      "[E1 B150]  Loss: 0.29869  Acc: 0.8765625  Time: 13.90  \n",
      "[E1 B160]  Loss: 0.28839  Acc: 0.8859375  Time: 13.87  \n",
      "[E1 B170]  Loss: 0.27942  Acc: 0.8890625  Time: 13.84  \n",
      "[E1 B180]  Loss: 0.27215  Acc: 0.89296875  Time: 13.89  \n",
      "[E1 B190]  Loss: 0.24228  Acc: 0.903125  Time: 13.88  \n",
      "[E1 B200]  Loss: 0.26862  Acc: 0.8921875  Time: 13.92  \n",
      "[E1 B210]  Loss: 0.23170  Acc: 0.909375  Time: 13.93  \n",
      "[E1 B220]  Loss: 0.24450  Acc: 0.90859375  Time: 13.93  \n",
      "[E1 B230]  Loss: 0.26106  Acc: 0.8984375  Time: 13.92  \n",
      "[E1 B240]  Loss: 0.24509  Acc: 0.8984375  Time: 13.90  \n",
      "[E1 B250]  Loss: 0.24550  Acc: 0.89921875  Time: 13.96  \n",
      "[E1 B260]  Loss: 0.23345  Acc: 0.91015625  Time: 13.94  \n",
      "[E1 B270]  Loss: 0.20615  Acc: 0.915625  Time: 13.97  \n",
      "[E1 B280]  Loss: 0.23898  Acc: 0.90703125  Time: 13.95  \n",
      "[E1 B290]  Loss: 0.23613  Acc: 0.91015625  Time: 13.97  \n",
      "[E1 B300]  Loss: 0.23169  Acc: 0.90703125  Time: 13.96  \n",
      "[E1 B310]  Loss: 0.20533  Acc: 0.9203125  Time: 13.93  \n",
      "[E1 B320]  Loss: 0.26263  Acc: 0.88984375  Time: 13.95  \n",
      "[E1 B330]  Loss: 0.21623  Acc: 0.91171875  Time: 13.94  \n",
      "[E1 B340]  Loss: 0.18969  Acc: 0.93203125  Time: 13.96  \n",
      "[E1 B350]  Loss: 0.22381  Acc: 0.91171875  Time: 13.96  \n",
      "[E1 B360]  Loss: 0.23051  Acc: 0.903125  Time: 13.91  \n",
      "[E1 B370]  Loss: 0.22332  Acc: 0.91015625  Time: 13.91  \n",
      "[E1 B380]  Loss: 0.22597  Acc: 0.9140625  Time: 13.96  \n",
      "[E1 B390]  Loss: 0.22611  Acc: 0.9078125  Time: 13.95  \n",
      "[E1 B400]  Loss: 0.20237  Acc: 0.9234375  Time: 13.94  \n",
      "[E1 B410]  Loss: 0.24225  Acc: 0.91015625  Time: 13.95  \n",
      "[E1 B420]  Loss: 0.23202  Acc: 0.9109375  Time: 13.94  \n",
      "[E1 B430]  Loss: 0.18681  Acc: 0.92890625  Time: 13.95  \n",
      "[E1 B440]  Loss: 0.20939  Acc: 0.925  Time: 13.93  \n",
      "[E1 B450]  Loss: 0.21282  Acc: 0.91796875  Time: 13.94  \n",
      "[E1 B460]  Loss: 0.22366  Acc: 0.9109375  Time: 13.94  \n",
      "[E1 B470]  Loss: 0.21771  Acc: 0.91328125  Time: 13.95  \n",
      "[E1 B480]  Loss: 0.19696  Acc: 0.9234375  Time: 13.94  \n",
      "[E1 B490]  Loss: 0.22893  Acc: 0.90859375  Time: 13.94  \n",
      "[E1 B500]  Loss: 0.19920  Acc: 0.92109375  Time: 13.89  \n",
      "[E1 B510]  Loss: 0.18740  Acc: 0.928125  Time: 13.90  \n",
      "[E1 B520]  Loss: 0.19824  Acc: 0.92421875  Time: 13.90  \n",
      "[Epoch 1] 890.24 seconds | Train acc: 0.7830 Train loss: 0.6263 Val acc: 0.7443 Val loss: 0.8274\n",
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-11 22:47:55.272441: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "Downloading: 28.7kB [00:00, 20.9MB/s]                   \n",
      "Downloading: 28.7kB [00:00, 25.2MB/s]                   \n",
      "Downloading: 100%|██████████| 7.44M/7.44M [00:00<00:00, 59.4MB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 852kB/s] \n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.46MB/s]\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 417kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [00:05<00:00, 86.3MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "python coms6998-project/run_trial.py \\\n",
    "  --name bert-base-uncased_sst2_128_10_fixed_01_1 \\\n",
    "  --transformer bert-base-uncased \\\n",
    "  --dataset sst2 \\\n",
    "  --batch-size 128 \\\n",
    "  --batch-logging 10 \\\n",
    "  --learning-scheme fixed \\\n",
    "  --learning-rate 0.01 \\\n",
    "  --epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MqMCRlyrsE9R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "coms6998-project-colab-bash-exec",
   "provenance": []
  },
  "environment": {
   "name": "common-cu101.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu101:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
