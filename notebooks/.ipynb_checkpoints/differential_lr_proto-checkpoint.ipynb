{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate GPU is available for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset):\n",
    "    ds = datasets.load_dataset('glue', dataset)\n",
    "    num_classes = ds['train'].features['label'].num_classes\n",
    "    return ds, num_classes\n",
    "\n",
    "\n",
    "def create_dataset_from_text_dataset(ds, tokenizer):\n",
    "    encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attn_masks = encoding['attention_mask']\n",
    "    labels = torch.tensor(ds['label'])\n",
    "    return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "\n",
    "def get_tensor_datasets(dataset_dict, splits, tokenizer):\n",
    "    split_datasets = {}\n",
    "    for s in splits:\n",
    "        split_datasets[s] = create_dataset_from_text_dataset(dataset_dict[s], tokenizer)\n",
    "    return split_datasets\n",
    "\n",
    "\n",
    "def get_data_loaders(split_datasets):\n",
    "    train_loader = DataLoader(split_datasets['train'], batch_size=256, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(split_datasets['validation'], batch_size=256, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = 'sst2'\n",
    "print(f'Loading {dataset} dataset...')\n",
    "dataset_dict, num_classes = get_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# sst2 = datasets.load_dataset('glue', 'sst2')\n",
    "# sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     print(sst2['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sst2['train'].features['label'].num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AdapterType\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "\n",
    "def get_tokenizer(model_name):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_transformer(model_name, num_labels, adapter, dataset):\n",
    "    if model_name == 'bert-base-uncased':\n",
    "        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        if adapter:\n",
    "            model.add_adapter(dataset, AdapterType.text_task)\n",
    "            model.train_adapter(dataset)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_criterion(num_labels):\n",
    "    if num_labels == 2:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "print(f'Loading tokenizer for {model_name}...')\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader for various splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for each split\n",
    "splits = list(dataset_dict.keys())\n",
    "print(f'Creating data loader for {splits} splits...')\n",
    "split_datasets = get_tensor_datasets(dataset_dict, splits, tokenizer)\n",
    "train_loader, val_loader = get_data_loaders(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched[2].size())\n",
    "    \n",
    "    if i_batch == 0:\n",
    "        print(sample_batched[2])\n",
    "        sb = sample_batched[2].to('cuda')\n",
    "        print(sb)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# def create_dataset_from_text_dataset(ds):\n",
    "#     encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "#     input_ids = encoding['input_ids']\n",
    "#     attn_masks = encoding['attention_mask']\n",
    "#     labels = torch.tensor(ds['label'])\n",
    "    \n",
    "#     return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "# splits = ['train',  'validation', 'test']\n",
    "# split_datasets = {}\n",
    "\n",
    "# for s in splits:\n",
    "#     split_datasets[s] = create_dataset_from_text_dataset(sst2[s])\n",
    "\n",
    "# split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data loader\n",
    "# sample_loader = DataLoader(split_datasets['train'], batch_size=3, shuffle=True)\n",
    "# for i in sample_loader:\n",
    "#     input_ids, attn_masks, labels = i\n",
    "#     decoded = tokenizer.batch_decode(input_ids)\n",
    "#     for d in decoded:\n",
    "#         print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "adapter = False\n",
    "print(f'Loading {model_name} with adapters={adapter}...')\n",
    "model = get_transformer(model_name,\n",
    "                        num_labels=num_classes,\n",
    "                        adapter=adapter,\n",
    "                        dataset=dataset)\n",
    "criterion = get_criterion(num_labels=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, child in model.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(add_adapters=False):\n",
    "#     model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "#     if add_adapters:\n",
    "#         from transformers import AdapterType\n",
    "#         model.add_adapter(\"sst2\", AdapterType.text_task)\n",
    "#         model.train_adapter(\"sst2\")\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(split_datasets['train'], batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(split_datasets['validation'], batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import logging\n",
    "# logging.set_verbosity_warning()\n",
    "\n",
    "# model = create_model(add_adapters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Learning Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_learning_scheme(learning_scheme, model):\n",
    "    if learning_scheme == 'differential':\n",
    "        optimizer_grouped_parameters = differential_learning_scheme(model)\n",
    "        optimizer = torch.optim.SGD(optimizer_grouped_parameters)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def differential_learning_scheme(model, learning_rate=0.1, divisor=2.6):\n",
    "    param_prefixes = {}\n",
    "    for n, p in model.named_parameters():\n",
    "        base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "        if base not in param_prefixes:\n",
    "            param_prefixes[base] = 0\n",
    "\n",
    "    param_prefix_divisors = list(reversed([divisor * i for i in range(1, len(param_prefixes))])) + [1]\n",
    "    param_learning_rates = [learning_rate / ld for ld in param_prefix_divisors]\n",
    "\n",
    "    param_prefix_lr_lookup = dict(zip(param_prefixes.keys(), param_learning_rates))\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': p, 'lr': param_prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "        for n, p in model.named_parameters()\n",
    "    ]\n",
    "\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learning scheme\n",
    "learning_scheme = 'differential'\n",
    "print(f'Configuring {learning_scheme} learning scheme...')\n",
    "optimizer = get_learning_scheme(learning_scheme, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefixes = {}\n",
    "# for n, p in model.named_parameters():\n",
    "#     base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "#     if base not in prefixes:\n",
    "#         prefixes[base] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_LR = 0.1\n",
    "# BASE_DIVISOR = 2.6\n",
    "\n",
    "# prefix_divisors = list(reversed([BASE_DIVISOR * i for i in range(1, len(prefixes))])) + [1]\n",
    "# layer_learning_rates = [BASE_LR / ld for ld in prefix_divisors]\n",
    "\n",
    "# prefix_lr_lookup = dict(zip(prefixes.keys(), layer_learning_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': p, 'lr': prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "#     for n, p in model.named_parameters()\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.SGD(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print('Using device:', device)\n",
    "model = model.to(device)\n",
    "n_epochs = 10\n",
    "optimizer = optimizer\n",
    "scheduler = None\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "N_MINI_BATCH_CHECK = 5\n",
    "\n",
    "def measure_performance(loader):\n",
    "    running_loss = 0.0\n",
    "    correct_count = 0.0\n",
    "    total_count = 0.0\n",
    "    for data in loader:\n",
    "        input_ids = data[0].to(device)\n",
    "        attn_masks = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "            loss = criterion(outputs, labels)\n",
    "            probas = F.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probas, axis=1)\n",
    "\n",
    "            # Track stats\n",
    "            running_loss += loss\n",
    "            correct_count += torch.sum(preds == labels)\n",
    "            total_count += len(labels)\n",
    "\n",
    "    running_loss /= len(loader)\n",
    "    acc = correct_count / total_count\n",
    "\n",
    "    return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_loader:\n",
    "    print('Initial evaluating on validation dataset')\n",
    "    val_loss, val_acc = measure_performance(val_loader)\n",
    "    epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "    print(epoch_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    print(f'--- Epoch: {epoch} ---')\n",
    "    epoch_start_time = time.time()\n",
    "    batch_start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        input_ids = data[0].to(device)\n",
    "        attn_masks = data[1].to(device)\n",
    "        labels = data[2].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Evaluation/optimization step\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Print statistics periodically\n",
    "        running_loss += loss.item()\n",
    "        if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "            batch_end_time = time.time()\n",
    "            total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "            print(\n",
    "                f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "                f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "                f'Time: {total_batch_time:.2f} ',\n",
    "                f'LR: {scheduler.get_last_lr()}' if scheduler else '')\n",
    "\n",
    "            # Reset statistics\n",
    "            batch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    total_epoch_time = epoch_end_time - epoch_start_time\n",
    "    epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "\n",
    "    if val_loader:\n",
    "        val_loss, val_acc = measure_performance(val_loader)\n",
    "        epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {val_loss:.4f}'\n",
    "\n",
    "    print(epoch_summary)\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import torch.nn.functional as F\n",
    "# import copy\n",
    "\n",
    "# class Trainer:\n",
    "#     def __init__(self, model, n_epochs, optimizer):\n",
    "#         self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "#         self.model = model.to(self.device)\n",
    "#         self.n_epochs = n_epochs\n",
    "#         self.optimizer = optimizer\n",
    "#         self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "# #         no_decay = ['bias', 'LayerNorm.weight']\n",
    "# #         optimizer_grouped_parameters = [\n",
    "# #             {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "# #             {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# #         ]\n",
    "# #         self.optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=1e-3, momentum=0.9)\n",
    "#         # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer)\n",
    "#         # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10000, T_mult=2)        \n",
    "\n",
    "#     def measure_performance(self, loader):\n",
    "#         running_loss = 0.0\n",
    "#         correct_count = 0\n",
    "#         total_count = 0\n",
    "#         for data in loader:\n",
    "#             input_ids = data[0].to(self.device)\n",
    "#             attn_masks = data[1].to(self.device)\n",
    "#             labels = data[2].to(self.device)\n",
    "#             with torch.no_grad():\n",
    "#                 outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#                 loss = self.criterion(outputs, labels)\n",
    "#                 probas = F.softmax(outputs, dim=1)\n",
    "#                 preds = torch.argmax(probas, axis=1)\n",
    "                \n",
    "#                 # Track stats\n",
    "#                 running_loss += loss\n",
    "#                 correct_count += torch.sum(preds == labels) \n",
    "#                 total_count += len(labels) \n",
    "        \n",
    "#         running_loss /= len(loader)\n",
    "#         acc = correct_count / total_count\n",
    "\n",
    "#         return running_loss, acc\n",
    "\n",
    "#     def train_loop(self, train_loader, val_loader=None):\n",
    "#         print('Starting training loop\\n\\n')\n",
    "#         N_MINI_BATCH_CHECK = 200\n",
    "\n",
    "#         if val_loader:\n",
    "#             print('Initial evaluating on validation dataset')\n",
    "#             val_loss, val_acc = self.measure_performance(val_loader)\n",
    "#             epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "#             print(epoch_summary)\n",
    "\n",
    "#         for epoch in range(self.n_epochs):\n",
    "#             print(f'--- Epoch: {epoch} ---')\n",
    "#             epoch_start_time = time.time()\n",
    "#             batch_start_time = time.time()\n",
    "#             running_loss = 0.0\n",
    "\n",
    "#             for i, data in enumerate(train_loader):\n",
    "#                 input_ids = data[0].to(self.device)\n",
    "#                 attn_masks = data[1].to(self.device)\n",
    "#                 labels = data[2].to(self.device)\n",
    "\n",
    "#                 self.optimizer.zero_grad()\n",
    "\n",
    "#                 # Evaluation/optimization step\n",
    "#                 outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#                 loss = self.criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 self.optimizer.step()\n",
    "#                 if self.scheduler:\n",
    "#                     self.scheduler.step()\n",
    "                \n",
    "#                 # Print statistics periodically\n",
    "#                 running_loss += loss.item()\n",
    "#                 if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "#                     batch_end_time = time.time()\n",
    "#                     total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "#                     print(\n",
    "#                         f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "#                         f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "#                         f'Time: {total_batch_time:.2f} ',\n",
    "#                         f'LR: {self.scheduler.get_last_lr()}' if self.scheduler else '')\n",
    "\n",
    "#                     # Reset statistics\n",
    "#                     batch_start_time = time.time()\n",
    "#                     running_loss = 0.0\n",
    "\n",
    "#             epoch_end_time = time.time()\n",
    "#             total_epoch_time = epoch_end_time - epoch_start_time\n",
    "#             epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "            \n",
    "#             if val_loader:\n",
    "#                 val_loss, val_acc = self.measure_performance(val_loader)\n",
    "#                 epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {loss:.4f}'\n",
    "\n",
    "#             print(epoch_summary)\n",
    "\n",
    "#         print('Finished training')\n",
    "\n",
    "#     def lr_test(self, train_loader, lrs=(-9, 2)):\n",
    "#         \"\"\"\n",
    "#         lrs = (min_lr, max_lr, factor_scale)\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Prepare LR-finder loop\n",
    "#         model = copy.deepcopy(self.model).to(self.device)\n",
    "#         min_lr, max_lr = lrs\n",
    "#         lrs = np.logspace(min_lr, max_lr, num=len(train_loader), endpoint=True)\n",
    "#         losses = []\n",
    "#         for i, data in enumerate(iter(train_loader)):\n",
    "#             curr_lr = lrs[i]\n",
    "#             optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr)\n",
    "\n",
    "#             input_ids = data[0].to(self.device)\n",
    "#             attn_masks = data[1].to(self.device)\n",
    "#             labels = data[2].to(self.device)\n",
    "\n",
    "#             # Evaluation/optimization step\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "#             loss = self.criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             losses.append(loss)\n",
    "\n",
    "#             if i % 100 == 0:\n",
    "#                 print(f'Step [{i}, {len(train_loader)}] | LR: {curr_lr:.4e} | Loss: {loss:.4f}')\n",
    "\n",
    "#         return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(model, 10, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train_loop(train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
