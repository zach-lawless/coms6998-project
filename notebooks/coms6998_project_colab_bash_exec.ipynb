{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coms6998-project-colab-bash-exec",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MHb3CsX60-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e77a2f6-e54d-4170-c25a-8892fe04634e"
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = f'git clone https://{user}:{password}@github.com/{repo_name}.git'\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: zach-lawless\n",
            "Password: ··········\n",
            "Repo name: zach-lawless/coms6998-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsi7cjTW8cs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ee9116-650d-4b5e-b010-aaae60b131e0"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Dec 11 22:47:32 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8    13W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZk1j0Hn7rN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7313308-e84e-4c7a-de23-08139a1b4c76"
      },
      "source": [
        "%%bash\n",
        "\n",
        "pip install -r coms6998-project/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "Collecting adapter-transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/44/1370c187aba1349d56d6813ec4de54644d15e154983050f4923ce5455069/adapter_transformers-1.1.0-py3-none-any.whl (1.3MB)\n",
            "Collecting xxhash\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (1.1.5)\n",
            "Collecting pyarrow>=0.17.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.8)\n",
            "Collecting sacremoses\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (20.7)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (3.0.12)\n",
            "Collecting tokenizers==0.9.3\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets->-r coms6998-project/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets->-r coms6998-project/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (50.3.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0c6bb4b2ca504f9e48caa40a65b1bcf0f5351fcc4844aa63e54ee7582986b226\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: xxhash, pyarrow, datasets, sacremoses, sentencepiece, tokenizers, adapter-transformers\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed adapter-transformers-1.1.0 datasets-1.1.3 pyarrow-2.0.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_BcPELe7kAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5d31f3-1014-4403-c96b-4b7b340655ec"
      },
      "source": [
        "%%bash\n",
        "\n",
        "python coms6998-project/run_trial.py --help"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: run_trial.py [-h] [--name NAME] [--transformer {bert-base-uncased}]\n",
            "                    [--dataset {sst2}] [--batch-size BATCH_SIZE]\n",
            "                    [--batch-logging BATCH_LOGGING] [--adapter ADAPTER]\n",
            "                    [--learning-scheme {differential,fixed,nesterov}]\n",
            "                    [--learning-rate LEARNING_RATE]\n",
            "                    [--max_learning_rate MAX_LEARNING_RATE] [--epochs EPOCHS]\n",
            "                    [--scheduler {cyclic-triangular}]\n",
            "\n",
            "Run a finetuning trial\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --name NAME           name to give trial (for output saving purposes)\n",
            "  --transformer {bert-base-uncased}\n",
            "                        the specific transformer to finetune\n",
            "  --dataset {sst2}      the dataset you are finetuning on\n",
            "  --batch-size BATCH_SIZE\n",
            "                        how large of batches to feed to the transformer\n",
            "  --batch-logging BATCH_LOGGING\n",
            "                        how frequently to log and print finetuning batch info\n",
            "  --adapter ADAPTER     whether to add adapters to the transformer or not\n",
            "  --learning-scheme {differential,fixed,nesterov}\n",
            "                        the learning scheme to fine tune with\n",
            "  --learning-rate LEARNING_RATE\n",
            "                        the learning rate to use for finetuning\n",
            "  --max_learning_rate MAX_LEARNING_RATE\n",
            "                        the max learning rate if using a scheduler\n",
            "  --epochs EPOCHS       how many epochs to finetune for\n",
            "  --scheduler {cyclic-triangular}\n",
            "                        learning rate scheduler to use, if any\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 22:47:51.126046: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HFBA4dy7Aqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f255b6-e00a-40b8-d9df-145d59e3dbe7"
      },
      "source": [
        "%%bash\n",
        "\n",
        "python coms6998-project/run_trial.py \\\n",
        "  --name bert-base-uncased_sst2_128_10_fixed_01_1 \\\n",
        "  --transformer bert-base-uncased \\\n",
        "  --dataset sst2 \\\n",
        "  --batch-size 128 \\\n",
        "  --batch-logging 10 \\\n",
        "  --learning-scheme fixed \\\n",
        "  --learning-rate 0.01 \\\n",
        "  --epochs 1"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading sst2 dataset...\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n",
            "Loading tokenizer for bert-base-uncased...\n",
            "Creating data loader for ['train', 'validation', 'test'] splits...\n",
            "Loading bert-base-uncased with adapters=None...\n",
            "Configuring fixed learning scheme...\n",
            "Setting up scheduler if any...\n",
            "Initializing Trainer object...\n",
            "Beginning finetuning...\n",
            "Starting training loop\n",
            "Initial evaluating on validation dataset\n",
            "[Epoch 0] | Train acc: 0.5578 Train loss: 0.7781 Val acc: 0.5092 Val loss: 0.8618\n",
            "--- Epoch: 1 ---\n",
            "[E1 B10]  Loss: 0.69451  Acc: 0.53359375  Time: 13.97  \n",
            "[E1 B20]  Loss: 0.66820  Acc: 0.56953125  Time: 13.97  \n",
            "[E1 B30]  Loss: 0.58556  Acc: 0.675  Time: 13.92  \n",
            "[E1 B40]  Loss: 0.48718  Acc: 0.76640625  Time: 13.86  \n",
            "[E1 B50]  Loss: 0.43754  Acc: 0.8109375  Time: 13.87  \n",
            "[E1 B60]  Loss: 0.42410  Acc: 0.82421875  Time: 13.91  \n",
            "[E1 B70]  Loss: 0.31838  Acc: 0.875  Time: 13.96  \n",
            "[E1 B80]  Loss: 0.38770  Acc: 0.83515625  Time: 13.95  \n",
            "[E1 B90]  Loss: 0.37710  Acc: 0.84453125  Time: 13.97  \n",
            "[E1 B100]  Loss: 0.30136  Acc: 0.8796875  Time: 13.98  \n",
            "[E1 B110]  Loss: 0.36502  Acc: 0.8546875  Time: 13.96  \n",
            "[E1 B120]  Loss: 0.29550  Acc: 0.88046875  Time: 13.97  \n",
            "[E1 B130]  Loss: 0.28010  Acc: 0.88359375  Time: 13.98  \n",
            "[E1 B140]  Loss: 0.26527  Acc: 0.89453125  Time: 13.96  \n",
            "[E1 B150]  Loss: 0.29869  Acc: 0.8765625  Time: 13.90  \n",
            "[E1 B160]  Loss: 0.28839  Acc: 0.8859375  Time: 13.87  \n",
            "[E1 B170]  Loss: 0.27942  Acc: 0.8890625  Time: 13.84  \n",
            "[E1 B180]  Loss: 0.27215  Acc: 0.89296875  Time: 13.89  \n",
            "[E1 B190]  Loss: 0.24228  Acc: 0.903125  Time: 13.88  \n",
            "[E1 B200]  Loss: 0.26862  Acc: 0.8921875  Time: 13.92  \n",
            "[E1 B210]  Loss: 0.23170  Acc: 0.909375  Time: 13.93  \n",
            "[E1 B220]  Loss: 0.24450  Acc: 0.90859375  Time: 13.93  \n",
            "[E1 B230]  Loss: 0.26106  Acc: 0.8984375  Time: 13.92  \n",
            "[E1 B240]  Loss: 0.24509  Acc: 0.8984375  Time: 13.90  \n",
            "[E1 B250]  Loss: 0.24550  Acc: 0.89921875  Time: 13.96  \n",
            "[E1 B260]  Loss: 0.23345  Acc: 0.91015625  Time: 13.94  \n",
            "[E1 B270]  Loss: 0.20615  Acc: 0.915625  Time: 13.97  \n",
            "[E1 B280]  Loss: 0.23898  Acc: 0.90703125  Time: 13.95  \n",
            "[E1 B290]  Loss: 0.23613  Acc: 0.91015625  Time: 13.97  \n",
            "[E1 B300]  Loss: 0.23169  Acc: 0.90703125  Time: 13.96  \n",
            "[E1 B310]  Loss: 0.20533  Acc: 0.9203125  Time: 13.93  \n",
            "[E1 B320]  Loss: 0.26263  Acc: 0.88984375  Time: 13.95  \n",
            "[E1 B330]  Loss: 0.21623  Acc: 0.91171875  Time: 13.94  \n",
            "[E1 B340]  Loss: 0.18969  Acc: 0.93203125  Time: 13.96  \n",
            "[E1 B350]  Loss: 0.22381  Acc: 0.91171875  Time: 13.96  \n",
            "[E1 B360]  Loss: 0.23051  Acc: 0.903125  Time: 13.91  \n",
            "[E1 B370]  Loss: 0.22332  Acc: 0.91015625  Time: 13.91  \n",
            "[E1 B380]  Loss: 0.22597  Acc: 0.9140625  Time: 13.96  \n",
            "[E1 B390]  Loss: 0.22611  Acc: 0.9078125  Time: 13.95  \n",
            "[E1 B400]  Loss: 0.20237  Acc: 0.9234375  Time: 13.94  \n",
            "[E1 B410]  Loss: 0.24225  Acc: 0.91015625  Time: 13.95  \n",
            "[E1 B420]  Loss: 0.23202  Acc: 0.9109375  Time: 13.94  \n",
            "[E1 B430]  Loss: 0.18681  Acc: 0.92890625  Time: 13.95  \n",
            "[E1 B440]  Loss: 0.20939  Acc: 0.925  Time: 13.93  \n",
            "[E1 B450]  Loss: 0.21282  Acc: 0.91796875  Time: 13.94  \n",
            "[E1 B460]  Loss: 0.22366  Acc: 0.9109375  Time: 13.94  \n",
            "[E1 B470]  Loss: 0.21771  Acc: 0.91328125  Time: 13.95  \n",
            "[E1 B480]  Loss: 0.19696  Acc: 0.9234375  Time: 13.94  \n",
            "[E1 B490]  Loss: 0.22893  Acc: 0.90859375  Time: 13.94  \n",
            "[E1 B500]  Loss: 0.19920  Acc: 0.92109375  Time: 13.89  \n",
            "[E1 B510]  Loss: 0.18740  Acc: 0.928125  Time: 13.90  \n",
            "[E1 B520]  Loss: 0.19824  Acc: 0.92421875  Time: 13.90  \n",
            "[Epoch 1] 890.24 seconds | Train acc: 0.7830 Train loss: 0.6263 Val acc: 0.7443 Val loss: 0.8274\n",
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 22:47:55.272441: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\rDownloading:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\rDownloading: 28.7kB [00:00, 20.9MB/s]                   \n",
            "\rDownloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]\rDownloading: 28.7kB [00:00, 25.2MB/s]                   \n",
            "\rDownloading:   0%|          | 0.00/7.44M [00:00<?, ?B/s]\rDownloading:  64%|██████▍   | 4.77M/7.44M [00:00<00:00, 47.7MB/s]\rDownloading: 100%|██████████| 7.44M/7.44M [00:00<00:00, 59.4MB/s]\n",
            "\r0 examples [00:00, ? examples/s]\r5220 examples [00:00, 52192.58 examples/s]\r10000 examples [00:00, 50616.41 examples/s]\r15245 examples [00:00, 51152.61 examples/s]\r20257 examples [00:00, 50837.04 examples/s]\r25316 examples [00:00, 50762.21 examples/s]\r30401 examples [00:00, 50785.88 examples/s]\r35857 examples [00:00, 51860.35 examples/s]\r41061 examples [00:00, 51913.91 examples/s]\r46456 examples [00:00, 52506.71 examples/s]\r51603 examples [00:01, 52189.88 examples/s]\r57003 examples [00:01, 52718.73 examples/s]\r62206 examples [00:01, 52508.17 examples/s]\r                                           \r\r0 examples [00:00, ? examples/s]\r                                \r\r0 examples [00:00, ? examples/s]\r                                \r\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading:  16%|█▌        | 36.9k/232k [00:00<00:00, 278kB/s]\rDownloading:  83%|████████▎ | 193k/232k [00:00<00:00, 359kB/s] \rDownloading: 100%|██████████| 232k/232k [00:00<00:00, 852kB/s]\n",
            "\rDownloading:   0%|          | 0.00/466k [00:00<?, ?B/s]\rDownloading:   8%|▊         | 36.9k/466k [00:00<00:01, 300kB/s]\rDownloading:  46%|████▌     | 213k/466k [00:00<00:00, 392kB/s] \rDownloading: 100%|██████████| 466k/466k [00:00<00:00, 1.46MB/s]\n",
            "\rDownloading:   0%|          | 0.00/433 [00:00<?, ?B/s]\rDownloading: 100%|██████████| 433/433 [00:00<00:00, 417kB/s]\n",
            "\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   1%|▏         | 5.63M/440M [00:00<00:07, 56.3MB/s]\rDownloading:   3%|▎         | 12.7M/440M [00:00<00:07, 60.0MB/s]\rDownloading:   5%|▍         | 21.0M/440M [00:00<00:06, 65.4MB/s]\rDownloading:   7%|▋         | 30.0M/440M [00:00<00:05, 71.2MB/s]\rDownloading:   9%|▉         | 39.8M/440M [00:00<00:05, 77.6MB/s]\rDownloading:  11%|█▏        | 49.7M/440M [00:00<00:04, 82.9MB/s]\rDownloading:  14%|█▎        | 59.5M/440M [00:00<00:04, 87.1MB/s]\rDownloading:  16%|█▌        | 69.4M/440M [00:00<00:04, 90.3MB/s]\rDownloading:  18%|█▊        | 79.3M/440M [00:00<00:03, 92.7MB/s]\rDownloading:  20%|██        | 88.5M/440M [00:01<00:04, 83.7MB/s]\rDownloading:  22%|██▏       | 97.3M/440M [00:01<00:04, 84.9MB/s]\rDownloading:  24%|██▍       | 107M/440M [00:01<00:03, 87.1MB/s] \rDownloading:  26%|██▋       | 116M/440M [00:01<00:03, 88.8MB/s]\rDownloading:  28%|██▊       | 125M/440M [00:01<00:03, 84.6MB/s]\rDownloading:  30%|███       | 133M/440M [00:01<00:03, 79.2MB/s]\rDownloading:  32%|███▏      | 141M/440M [00:01<00:03, 77.3MB/s]\rDownloading:  34%|███▍      | 149M/440M [00:01<00:03, 76.2MB/s]\rDownloading:  36%|███▌      | 157M/440M [00:01<00:03, 76.7MB/s]\rDownloading:  38%|███▊      | 165M/440M [00:01<00:03, 78.9MB/s]\rDownloading:  39%|███▉      | 174M/440M [00:02<00:03, 79.9MB/s]\rDownloading:  42%|████▏     | 183M/440M [00:02<00:03, 83.3MB/s]\rDownloading:  44%|████▎     | 193M/440M [00:02<00:02, 87.0MB/s]\rDownloading:  46%|████▌     | 202M/440M [00:02<00:02, 89.0MB/s]\rDownloading:  48%|████▊     | 212M/440M [00:02<00:02, 91.9MB/s]\rDownloading:  50%|█████     | 221M/440M [00:02<00:02, 86.2MB/s]\rDownloading:  52%|█████▏    | 230M/440M [00:02<00:02, 87.6MB/s]\rDownloading:  54%|█████▍    | 240M/440M [00:02<00:02, 89.6MB/s]\rDownloading:  57%|█████▋    | 249M/440M [00:02<00:02, 90.8MB/s]\rDownloading:  59%|█████▊    | 258M/440M [00:03<00:02, 89.6MB/s]\rDownloading:  61%|██████    | 267M/440M [00:03<00:01, 88.1MB/s]\rDownloading:  63%|██████▎   | 276M/440M [00:03<00:01, 84.1MB/s]\rDownloading:  65%|██████▍   | 285M/440M [00:03<00:01, 82.1MB/s]\rDownloading:  67%|██████▋   | 293M/440M [00:03<00:01, 82.7MB/s]\rDownloading:  69%|██████▉   | 303M/440M [00:03<00:01, 86.8MB/s]\rDownloading:  71%|███████   | 313M/440M [00:03<00:01, 90.1MB/s]\rDownloading:  73%|███████▎  | 323M/440M [00:03<00:01, 92.7MB/s]\rDownloading:  76%|███████▌  | 333M/440M [00:03<00:01, 94.2MB/s]\rDownloading:  78%|███████▊  | 343M/440M [00:03<00:01, 95.8MB/s]\rDownloading:  80%|███████▉  | 352M/440M [00:04<00:00, 94.9MB/s]\rDownloading:  82%|████████▏ | 362M/440M [00:04<00:00, 84.6MB/s]\rDownloading:  84%|████████▍ | 370M/440M [00:04<00:00, 83.2MB/s]\rDownloading:  86%|████████▌ | 379M/440M [00:04<00:00, 84.7MB/s]\rDownloading:  88%|████████▊ | 389M/440M [00:04<00:00, 87.7MB/s]\rDownloading:  90%|█████████ | 398M/440M [00:04<00:00, 85.0MB/s]\rDownloading:  92%|█████████▏| 407M/440M [00:04<00:00, 86.1MB/s]\rDownloading:  94%|█████████▍| 416M/440M [00:04<00:00, 87.2MB/s]\rDownloading:  96%|█████████▋| 424M/440M [00:04<00:00, 86.4MB/s]\rDownloading:  98%|█████████▊| 433M/440M [00:05<00:00, 87.5MB/s]\rDownloading: 100%|██████████| 440M/440M [00:05<00:00, 86.3MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqMCRlyrsE9R"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}