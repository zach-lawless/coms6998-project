{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coms6998-project-colab-bash-exec",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MHb3CsX60-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577022ea-def0-41d1-8c2b-e417a2322a93"
      },
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "import urllib\n",
        "\n",
        "user = input('User name: ')\n",
        "password = getpass('Password: ')\n",
        "password = urllib.parse.quote(password) # your password is converted into url format\n",
        "repo_name = input('Repo name: ')\n",
        "\n",
        "cmd_string = f'git clone https://{user}:{password}@github.com/{repo_name}.git'\n",
        "\n",
        "os.system(cmd_string)\n",
        "cmd_string, password = \"\", \"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User name: zach-lawless\n",
            "Password: ··········\n",
            "Repo name: zach-lawless/coms6998-project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsi7cjTW8cs4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b57757-6caf-4a64-d3df-54c6e3b85c75"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Dec 11 18:05:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZk1j0Hn7rN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ae26c15-1602-41e7-ef44-2dfbfe421699"
      },
      "source": [
        "%%bash\n",
        "\n",
        "pip install -r coms6998-project/requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "Collecting adapter-transformers\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/44/1370c187aba1349d56d6813ec4de54644d15e154983050f4923ce5455069/adapter_transformers-1.1.0-py3-none-any.whl (1.3MB)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.3.3)\n",
            "Collecting xxhash\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting pyarrow>=0.17.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets->-r coms6998-project/requirements.txt (line 1)) (0.70.11.1)\n",
            "Collecting sacremoses\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "Collecting tokenizers==0.9.3\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (20.7)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (2019.12.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets->-r coms6998-project/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets->-r coms6998-project/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets->-r coms6998-project/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (50.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers->-r coms6998-project/requirements.txt (line 2)) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=27385c9959fc14827fa2e78fa02cde15d0cd86b250b7bc1b2982b62c5d91b140\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: xxhash, pyarrow, datasets, sacremoses, tokenizers, sentencepiece, adapter-transformers\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed adapter-transformers-1.1.0 datasets-1.1.3 pyarrow-2.0.0 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_BcPELe7kAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99432ba5-585c-426c-b941-49338c44bec2"
      },
      "source": [
        "%%bash\n",
        "\n",
        "python coms6998-project/run_trial.py --help"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: run_trial.py [-h] [--transformer {bert-base-uncased}]\n",
            "                    [--dataset {sst2}] [--batch-size BATCH_SIZE]\n",
            "                    [--adapter ADAPTER]\n",
            "                    [--learning-scheme {differential,fixed,nesterov}]\n",
            "                    [--learning-rate LEARNING_RATE]\n",
            "                    [--max_learning_rate MAX_LEARNING_RATE] [--epochs EPOCHS]\n",
            "                    [--scheduler {cyclic-triangular}]\n",
            "\n",
            "Run a finetuning trial\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --transformer {bert-base-uncased}\n",
            "                        the specific transformer to finetune\n",
            "  --dataset {sst2}      the dataset you are finetuning on\n",
            "  --batch-size BATCH_SIZE\n",
            "                        how large of batches to feed to the transformer\n",
            "  --adapter ADAPTER     whether to add adapters to the transformer or not\n",
            "  --learning-scheme {differential,fixed,nesterov}\n",
            "                        the learning scheme to fine tune with\n",
            "  --learning-rate LEARNING_RATE\n",
            "                        the learning rate to use for finetuning\n",
            "  --max_learning_rate MAX_LEARNING_RATE\n",
            "                        the max learning rate if using a scheduler\n",
            "  --epochs EPOCHS       how many epochs to finetune for\n",
            "  --scheduler {cyclic-triangular}\n",
            "                        learning rate scheduler to use, if any\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 18:06:00.945261: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HFBA4dy7Aqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f0891a-01a4-4225-9293-61f56d3c4fcd"
      },
      "source": [
        "%%bash\n",
        "\n",
        "python coms6998-project/run_trial.py \\\n",
        "  --transformer bert-base-uncased \\\n",
        "  --dataset sst2 \\\n",
        "  --batch-size 128 \\\n",
        "  --learning-scheme fixed \\\n",
        "  --learning-rate 0.01 \\\n",
        "  --epochs 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading sst2 dataset...\n",
            "Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4...\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4. Subsequent calls will reuse this data.\n",
            "Loading tokenizer for bert-base-uncased...\n",
            "Creating data loader for ['train', 'validation', 'test'] splits...\n",
            "Loading bert-base-uncased with adapters=None...\n",
            "Configuring fixed learning scheme...\n",
            "Setting up scheduler if any...\n",
            "Initializing Trainer object...\n",
            "Beginning finetuning...\n",
            "Starting training loop\n",
            "\n",
            "\n",
            "Initial evaluating on validation dataset\n",
            "[Epoch 0] | Val acc: 0.4897 Val loss: 0.7619\n",
            "\n",
            "\n",
            "--- Epoch: 0 ---\n",
            "[E1 B200]  Loss: 0.42451  Time: 245.73  \n",
            "[E1 B400]  Loss: 0.23627  Time: 258.72  \n",
            "[Epoch 1] 668.7510621547699 seconds | Val acc: 0.9060 | Val loss: 0.2277\n",
            "--- Epoch: 1 ---\n",
            "[E2 B200]  Loss: 0.17154  Time: 261.79  \n",
            "[E2 B400]  Loss: 0.16144  Time: 262.09  \n",
            "[Epoch 2] 688.3829514980316 seconds | Val acc: 0.9174 | Val loss: 0.2275\n",
            "Finished training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-12-11 18:06:08.516233: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\rDownloading:   0%|          | 0.00/7.83k [00:00<?, ?B/s]\rDownloading: 28.7kB [00:00, 23.2MB/s]                   \n",
            "\rDownloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]\rDownloading: 28.7kB [00:00, 27.2MB/s]                   \n",
            "\rDownloading:   0%|          | 0.00/7.44M [00:00<?, ?B/s]\rDownloading:  62%|██████▏   | 4.60M/7.44M [00:00<00:00, 46.0MB/s]\rDownloading: 100%|██████████| 7.44M/7.44M [00:00<00:00, 54.0MB/s]\n",
            "\r0 examples [00:00, ? examples/s]\r4923 examples [00:00, 49226.41 examples/s]\r9930 examples [00:00, 49472.23 examples/s]\r14404 examples [00:00, 47947.57 examples/s]\r19449 examples [00:00, 48670.44 examples/s]\r24281 examples [00:00, 48562.94 examples/s]\r29331 examples [00:00, 49126.92 examples/s]\r33895 examples [00:00, 48026.10 examples/s]\r38984 examples [00:00, 48850.52 examples/s]\r43848 examples [00:00, 48787.12 examples/s]\r48975 examples [00:01, 49504.56 examples/s]\r53811 examples [00:01, 49153.45 examples/s]\r58945 examples [00:01, 49788.89 examples/s]\r63881 examples [00:01, 49658.71 examples/s]\r                                           \r\r0 examples [00:00, ? examples/s]\r                                \r\r0 examples [00:00, ? examples/s]\r                                \r\rDownloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\rDownloading:  16%|█▌        | 36.9k/232k [00:00<00:00, 269kB/s]\rDownloading:  83%|████████▎ | 193k/232k [00:00<00:00, 348kB/s] \rDownloading: 100%|██████████| 232k/232k [00:00<00:00, 828kB/s]\n",
            "\rDownloading:   0%|          | 0.00/466k [00:00<?, ?B/s]\rDownloading:   8%|▊         | 36.9k/466k [00:00<00:01, 299kB/s]\rDownloading:  41%|████▏     | 193k/466k [00:00<00:00, 386kB/s] \rDownloading: 100%|██████████| 466k/466k [00:00<00:00, 1.46MB/s]\n",
            "\rDownloading:   0%|          | 0.00/433 [00:00<?, ?B/s]\rDownloading: 100%|██████████| 433/433 [00:00<00:00, 513kB/s]\n",
            "\rDownloading:   0%|          | 0.00/440M [00:00<?, ?B/s]\rDownloading:   0%|          | 1.29M/440M [00:00<00:34, 12.9MB/s]\rDownloading:   2%|▏         | 9.62M/440M [00:00<00:25, 17.2MB/s]\rDownloading:   4%|▍         | 18.0M/440M [00:00<00:18, 22.6MB/s]\rDownloading:   6%|▌         | 25.5M/440M [00:00<00:14, 28.7MB/s]\rDownloading:   7%|▋         | 32.3M/440M [00:00<00:11, 34.6MB/s]\rDownloading:   9%|▉         | 39.0M/440M [00:00<00:09, 40.5MB/s]\rDownloading:  10%|█         | 45.1M/440M [00:00<00:09, 41.3MB/s]\rDownloading:  12%|█▏        | 52.6M/440M [00:00<00:08, 47.8MB/s]\rDownloading:  14%|█▎        | 59.8M/440M [00:00<00:07, 53.2MB/s]\rDownloading:  15%|█▌        | 67.3M/440M [00:01<00:06, 58.2MB/s]\rDownloading:  17%|█▋        | 74.6M/440M [00:01<00:06, 59.7MB/s]\rDownloading:  18%|█▊        | 81.3M/440M [00:01<00:05, 61.7MB/s]\rDownloading:  20%|██        | 89.3M/440M [00:01<00:05, 66.2MB/s]\rDownloading:  22%|██▏       | 97.2M/440M [00:01<00:04, 69.8MB/s]\rDownloading:  24%|██▍       | 106M/440M [00:01<00:04, 73.8MB/s] \rDownloading:  26%|██▌       | 113M/440M [00:01<00:04, 72.4MB/s]\rDownloading:  28%|██▊       | 121M/440M [00:01<00:04, 73.8MB/s]\rDownloading:  29%|██▉       | 129M/440M [00:01<00:04, 64.8MB/s]\rDownloading:  31%|███       | 137M/440M [00:02<00:04, 69.3MB/s]\rDownloading:  33%|███▎      | 146M/440M [00:02<00:03, 73.9MB/s]\rDownloading:  35%|███▌      | 155M/440M [00:02<00:03, 78.0MB/s]\rDownloading:  37%|███▋      | 163M/440M [00:02<00:03, 78.2MB/s]\rDownloading:  39%|███▉      | 171M/440M [00:02<00:03, 78.5MB/s]\rDownloading:  41%|████      | 179M/440M [00:02<00:03, 79.6MB/s]\rDownloading:  42%|████▏     | 187M/440M [00:02<00:03, 77.9MB/s]\rDownloading:  44%|████▍     | 195M/440M [00:02<00:03, 75.9MB/s]\rDownloading:  46%|████▌     | 203M/440M [00:02<00:03, 77.9MB/s]\rDownloading:  48%|████▊     | 211M/440M [00:02<00:02, 78.7MB/s]\rDownloading:  50%|████▉     | 219M/440M [00:03<00:02, 76.1MB/s]\rDownloading:  52%|█████▏    | 227M/440M [00:03<00:03, 70.2MB/s]\rDownloading:  53%|█████▎    | 234M/440M [00:03<00:03, 64.1MB/s]\rDownloading:  55%|█████▍    | 242M/440M [00:03<00:02, 68.1MB/s]\rDownloading:  57%|█████▋    | 249M/440M [00:03<00:02, 67.0MB/s]\rDownloading:  58%|█████▊    | 256M/440M [00:03<00:02, 67.3MB/s]\rDownloading:  60%|█████▉    | 263M/440M [00:03<00:02, 64.6MB/s]\rDownloading:  62%|██████▏   | 271M/440M [00:03<00:02, 69.8MB/s]\rDownloading:  63%|██████▎   | 279M/440M [00:03<00:02, 62.8MB/s]\rDownloading:  65%|██████▍   | 286M/440M [00:04<00:02, 65.4MB/s]\rDownloading:  67%|██████▋   | 294M/440M [00:04<00:02, 69.3MB/s]\rDownloading:  68%|██████▊   | 301M/440M [00:04<00:02, 66.6MB/s]\rDownloading:  70%|██████▉   | 308M/440M [00:04<00:02, 66.2MB/s]\rDownloading:  71%|███████▏  | 315M/440M [00:04<00:02, 61.5MB/s]\rDownloading:  73%|███████▎  | 321M/440M [00:04<00:01, 63.7MB/s]\rDownloading:  75%|███████▍  | 329M/440M [00:04<00:01, 66.4MB/s]\rDownloading:  76%|███████▌  | 336M/440M [00:04<00:01, 67.1MB/s]\rDownloading:  78%|███████▊  | 343M/440M [00:04<00:01, 66.7MB/s]\rDownloading:  79%|███████▉  | 349M/440M [00:05<00:01, 67.4MB/s]\rDownloading:  81%|████████  | 356M/440M [00:05<00:01, 66.7MB/s]\rDownloading:  83%|████████▎ | 364M/440M [00:05<00:01, 69.9MB/s]\rDownloading:  84%|████████▍ | 371M/440M [00:05<00:00, 70.3MB/s]\rDownloading:  86%|████████▌ | 378M/440M [00:05<00:00, 69.3MB/s]\rDownloading:  88%|████████▊ | 387M/440M [00:05<00:00, 72.9MB/s]\rDownloading:  90%|████████▉ | 395M/440M [00:05<00:00, 75.6MB/s]\rDownloading:  91%|█████████▏| 403M/440M [00:05<00:00, 73.9MB/s]\rDownloading:  93%|█████████▎| 410M/440M [00:05<00:00, 62.9MB/s]\rDownloading:  95%|█████████▍| 417M/440M [00:06<00:00, 64.1MB/s]\rDownloading:  96%|█████████▌| 423M/440M [00:06<00:00, 55.1MB/s]\rDownloading:  98%|█████████▊| 431M/440M [00:06<00:00, 59.6MB/s]\rDownloading: 100%|█████████▉| 439M/440M [00:06<00:00, 64.2MB/s]\rDownloading: 100%|██████████| 440M/440M [00:06<00:00, 68.9MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}