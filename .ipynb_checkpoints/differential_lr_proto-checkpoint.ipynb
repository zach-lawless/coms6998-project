{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate GPU is available for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  9 14:40:04 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.06    Driver Version: 450.51.06    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U adapter-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/sst2/1.0.0/7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "sst2 = datasets.load_dataset('glue', 'sst2')\n",
    "sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0, 'label': 0, 'sentence': 'hide new secretions from the parental units '}\n",
      "{'idx': 1, 'label': 0, 'sentence': 'contains no wit , only labored gags '}\n",
      "{'idx': 2, 'label': 1, 'sentence': 'that loves its characters and communicates something rather beautiful about human nature '}\n",
      "{'idx': 3, 'label': 0, 'sentence': 'remains utterly satisfied to remain the same throughout '}\n",
      "{'idx': 4, 'label': 0, 'sentence': 'on the worst revenge-of-the-nerds clich√©s the filmmakers could dredge up '}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(sst2['train'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loader for various splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataset.TensorDataset at 0x7f57f1bba4d0>,\n",
       " 'validation': <torch.utils.data.dataset.TensorDataset at 0x7f57f1bba690>,\n",
       " 'test': <torch.utils.data.dataset.TensorDataset at 0x7f57f1bba810>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_dataset_from_text_dataset(ds):\n",
    "    encoding = tokenizer(ds['sentence'], return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = encoding['input_ids']\n",
    "    attn_masks = encoding['attention_mask']\n",
    "    labels = torch.tensor(ds['label'])\n",
    "    \n",
    "    return TensorDataset(input_ids, attn_masks, labels)\n",
    "\n",
    "splits = ['train',  'validation', 'test']\n",
    "split_datasets = {}\n",
    "\n",
    "for s in splits:\n",
    "    split_datasets[s] = create_dataset_from_text_dataset(sst2[s])\n",
    "\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data loader\n",
    "# sample_loader = DataLoader(split_datasets['train'], batch_size=3, shuffle=True)\n",
    "# for i in sample_loader:\n",
    "#     input_ids, attn_masks, labels = i\n",
    "#     decoded = tokenizer.batch_decode(input_ids)\n",
    "#     for d in decoded:\n",
    "#         print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(add_adapters=False):\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    if add_adapters:\n",
    "        from transformers import AdapterType\n",
    "        model.add_adapter(\"sst2\", AdapterType.text_task)\n",
    "        model.train_adapter(\"sst2\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(split_datasets['train'], batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(split_datasets['validation'], batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "model = create_model(add_adapters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = {}\n",
    "for n, p in model.named_parameters():\n",
    "    base = n.partition('.weight')[0].partition('.bias')[0]\n",
    "    if base not in prefixes:\n",
    "        prefixes[base] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR = 0.1\n",
    "BASE_DIVISOR = 2.6\n",
    "\n",
    "prefix_divisors = list(reversed([BASE_DIVISOR * i for i in range(1, len(prefixes))])) + [1]\n",
    "layer_learning_rates = [BASE_LR / ld for ld in prefix_divisors]\n",
    "\n",
    "prefix_lr_lookup = dict(zip(prefixes.keys(), layer_learning_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {'params': p, 'lr': prefix_lr_lookup[n.partition('.weight')[0].partition('.bias')[0]]}\n",
    "    for n, p in model.named_parameters()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, n_epochs, optimizer):\n",
    "        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "#         no_decay = ['bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#             {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#             {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#         ]\n",
    "#         self.optimizer = torch.optim.SGD(optimizer_grouped_parameters, lr=1e-3, momentum=0.9)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer)\n",
    "        # self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10000, T_mult=2)        \n",
    "\n",
    "    def measure_performance(self, loader):\n",
    "        running_loss = 0.0\n",
    "        correct_count = 0\n",
    "        total_count = 0\n",
    "        for data in loader:\n",
    "            input_ids = data[0].to(self.device)\n",
    "            attn_masks = data[1].to(self.device)\n",
    "            labels = data[2].to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                probas = F.softmax(outputs, dim=1)\n",
    "                preds = torch.argmax(probas, axis=1)\n",
    "                \n",
    "                # Track stats\n",
    "                running_loss += loss\n",
    "                correct_count += torch.sum(preds == labels) \n",
    "                total_count += len(labels) \n",
    "        \n",
    "        running_loss /= len(loader)\n",
    "        acc = correct_count / total_count\n",
    "\n",
    "        return running_loss, acc\n",
    "\n",
    "    def train_loop(self, train_loader, val_loader=None):\n",
    "        print('Starting training loop\\n\\n')\n",
    "        N_MINI_BATCH_CHECK = 200\n",
    "\n",
    "        if val_loader:\n",
    "            print('Initial evaluating on validation dataset')\n",
    "            val_loss, val_acc = self.measure_performance(val_loader)\n",
    "            epoch_summary = f'[Epoch 0] | Val acc: {val_acc:.4f} Val loss: {val_loss:.4f}\\n\\n'\n",
    "            print(epoch_summary)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            print(f'--- Epoch: {epoch} ---')\n",
    "            epoch_start_time = time.time()\n",
    "            batch_start_time = time.time()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for i, data in enumerate(iter(train_loader)):\n",
    "                input_ids = data[0].to(self.device)\n",
    "                attn_masks = data[1].to(self.device)\n",
    "                labels = data[2].to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Evaluation/optimization step\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if self.scheduler:\n",
    "                    self.scheduler.step()\n",
    "                \n",
    "                # Print statistics periodically\n",
    "                running_loss += loss.item()\n",
    "                if i % N_MINI_BATCH_CHECK == N_MINI_BATCH_CHECK - 1:\n",
    "                    batch_end_time = time.time()\n",
    "                    total_batch_time = batch_end_time - batch_start_time\n",
    "\n",
    "                    print(\n",
    "                        f'[E{epoch + 1:d} B{i + 1:d}] ',\n",
    "                        f'Loss: {running_loss / N_MINI_BATCH_CHECK:.5f} ',\n",
    "                        f'Time: {total_batch_time:.2f} ',\n",
    "                        f'LR: {self.scheduler.get_last_lr()}' if self.scheduler else '')\n",
    "\n",
    "                    # Reset statistics\n",
    "                    batch_start_time = time.time()\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            total_epoch_time = epoch_end_time - epoch_start_time\n",
    "            epoch_summary = '[Epoch {}] {} seconds'.format((epoch + 1), total_epoch_time)\n",
    "            \n",
    "            if val_loader:\n",
    "                val_loss, val_acc = self.measure_performance(val_loader)\n",
    "                epoch_summary += f' | Val acc: {val_acc:.4f} | Val loss: {loss:.4f}'\n",
    "\n",
    "            print(epoch_summary)\n",
    "\n",
    "        print('Finished training')\n",
    "\n",
    "    def lr_test(self, train_loader, lrs=(-9, 2)):\n",
    "        \"\"\"\n",
    "        lrs = (min_lr, max_lr, factor_scale)\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepare LR-finder loop\n",
    "        model = copy.deepcopy(self.model).to(self.device)\n",
    "        min_lr, max_lr = lrs\n",
    "        lrs = np.logspace(min_lr, max_lr, num=len(train_loader), endpoint=True)\n",
    "        losses = []\n",
    "        for i, data in enumerate(iter(train_loader)):\n",
    "            curr_lr = lrs[i]\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=curr_lr)\n",
    "\n",
    "            input_ids = data[0].to(self.device)\n",
    "            attn_masks = data[1].to(self.device)\n",
    "            labels = data[2].to(self.device)\n",
    "\n",
    "            # Evaluation/optimization step\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attn_masks)[0]\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f'Step [{i}, {len(train_loader)}] | LR: {curr_lr:.4e} | Loss: {loss:.4f}')\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, 10, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-507ea09403a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train_loop(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
